1
00:00:02,730 --> 00:00:03,730
- Mark, I gotta be honest,

2
00:00:03,630 --> 00:00:04,630
not long ago I was thinking
we may be doing this

3
00:00:06,060 --> 00:00:07,060
as like a post-fight
interview in Las Vegas,

4
00:00:09,180 --> 00:00:10,180
right outside of the Octagon

5
00:00:10,740 --> 00:00:11,740
after you get out of a fight with Elon.

6
00:00:13,170 --> 00:00:14,170
- Maybe next year.

7
00:00:14,130 --> 00:00:15,130
- Maybe next year.

8
00:00:15,560 --> 00:00:16,560
- No, not Elon, but, but someone,

9
00:00:16,611 --> 00:00:17,611
I wanna keep competing,

10
00:00:17,880 --> 00:00:18,880
but I just need to find someone to ask.

11
00:00:20,557 --> 00:00:21,557
- Do you think he was ever
serious about fighting?

12
00:00:21,660 --> 00:00:22,660
- I don't know, you'd have to ask him.

13
00:00:23,220 --> 00:00:24,220
But I don't know,

14
00:00:24,053 --> 00:00:25,053
I mean this is like a thing that just,

15
00:00:25,650 --> 00:00:26,650
I really enjoy doing it as a sport so-

16
00:00:27,540 --> 00:00:28,540
- Yeah.

17
00:00:28,717 --> 00:00:29,717
- So for me there's a sort
of level of it's competition.

18
00:00:34,500 --> 00:00:35,500
It's a sport.

19
00:00:35,333 --> 00:00:36,333
And so, I mean, I love doing it.

20
00:00:37,980 --> 00:00:38,980
I trained with a bunch of guys

21
00:00:40,170 --> 00:00:41,170
and I definitely want
to compete more, but-

22
00:00:44,056 --> 00:00:45,056
- Are there any other tech CO rivals

23
00:00:46,740 --> 00:00:47,740
you would wanna fight if you could or-

24
00:00:48,930 --> 00:00:49,930
- No, I think it'll be
more fun to fight someone

25
00:00:52,637 --> 00:00:53,637
who actually fights.

26
00:00:54,180 --> 00:00:55,180
- Takes it seriously.

27
00:00:55,013 --> 00:00:56,013
- Yeah.

28
00:00:56,637 --> 00:00:57,637
- Yeah.

29
00:00:57,470 --> 00:00:58,470
- Yeah, so this is like their, yeah.

30
00:00:58,370 --> 00:00:59,370
- So settling tech business
rivalries by combat,

31
00:01:02,340 --> 00:01:03,340
you don't think that's gonna
become like a thing now?

32
00:01:04,710 --> 00:01:05,710
- No, I don't think so.

33
00:01:06,300 --> 00:01:07,300
I think that's not generally the direction

34
00:01:08,040 --> 00:01:09,040
that our society is heading.

35
00:01:10,899 --> 00:01:11,899
- It's probably for the best.

36
00:01:11,896 --> 00:01:12,896
- It probably is for the best.

37
00:01:12,729 --> 00:01:13,729
I think a little bit of a channel

38
00:01:15,120 --> 00:01:16,120
to get some aggression out is good.

39
00:01:17,251 --> 00:01:18,251
- Yeah.

40
00:01:18,259 --> 00:01:19,259
- I think the one that
was proposed with Elon

41
00:01:21,900 --> 00:01:22,900
could have been fun, but it's okay.

42
00:01:22,803 --> 00:01:23,803
- Well I guess what I'm saying
is like, if he told you,

43
00:01:24,840 --> 00:01:25,840
if he came back to you and said,

44
00:01:26,010 --> 00:01:27,010
I'll fight on your terms,
you pick the venue,

45
00:01:28,477 --> 00:01:29,477
would you still do it?

46
00:01:30,420 --> 00:01:31,420
- I don't think it'll happen.

47
00:01:31,757 --> 00:01:32,757
- Okay.

48
00:01:32,899 --> 00:01:33,899
- I don't think it'll happen.

49
00:01:33,916 --> 00:01:34,916
- Okay.

50
00:01:34,919 --> 00:01:35,919
Fair, I agree with you.

51
00:01:35,917 --> 00:01:36,917
- Yeah, I just think that it's like,

52
00:01:37,620 --> 00:01:38,620
there's sort of a valorization

53
00:01:39,540 --> 00:01:40,540
where people look at this stuff

54
00:01:40,710 --> 00:01:41,710
and are like, oh, I could do that.

55
00:01:42,210 --> 00:01:43,210
But I mean, you have to train, you know?

56
00:01:44,259 --> 00:01:45,259
- Yeah.

57
00:01:45,277 --> 00:01:46,277
- It's like you wanna,
it's very technical.

58
00:01:46,110 --> 00:01:47,110
It's very fun.

59
00:01:46,943 --> 00:01:47,943
- Yeah.

60
00:01:48,339 --> 00:01:49,339
- Very intellectual.

61
00:01:49,354 --> 00:01:50,354
I mean, I used to, when
I was a lot younger,

62
00:01:50,760 --> 00:01:51,760
I used to fence competitively

63
00:01:52,290 --> 00:01:53,290
and a lot of the striking aspects,

64
00:01:56,130 --> 00:01:57,130
I mean obviously it's different
cuz I've been fencing,

65
00:01:57,870 --> 00:01:58,870
you're playing for points, right?

66
00:01:59,270 --> 00:02:00,270
So when you get a touch, the point is,

67
00:02:00,957 --> 00:02:01,957
and the sequence is done.

68
00:02:03,437 --> 00:02:04,437
Whereas here you have to worry about

69
00:02:04,320 --> 00:02:05,320
being countered and all that.

70
00:02:05,460 --> 00:02:06,460
But it's very intellectual.

71
00:02:09,690 --> 00:02:10,690
I used to, I really enjoyed, you know,

72
00:02:13,000 --> 00:02:14,000
thinking about all the different combos

73
00:02:14,296 --> 00:02:15,296
and moves and all that.

74
00:02:15,129 --> 00:02:16,129
And there's a period
where you're ramping up

75
00:02:17,667 --> 00:02:18,667
and like learning all the basic stuff

76
00:02:20,460 --> 00:02:21,460
before you can really get to
the intellectual part of it.

77
00:02:23,100 --> 00:02:24,100
But once you're there,

78
00:02:24,690 --> 00:02:25,690
I don't know, it's super fun.

79
00:02:25,650 --> 00:02:26,650
I love doing it with friends and it's-

80
00:02:27,030 --> 00:02:28,030
- So your mind isn't just like shut off

81
00:02:28,560 --> 00:02:29,560
when you're doing it?

82
00:02:29,619 --> 00:02:30,619
- No.

83
00:02:30,616 --> 00:02:31,616
- Like you actually find it
to be mentally stimulating.

84
00:02:31,590 --> 00:02:32,590
- Yeah.

85
00:02:33,037 --> 00:02:34,037
- Interesting.

86
00:02:33,870 --> 00:02:34,870
Last year when Elon was
close to taking over Twitter,

87
00:02:36,900 --> 00:02:37,900
I asked you for if you
had any advice for him.

88
00:02:38,820 --> 00:02:39,820
I'm not gonna ask you to
give him advice this time,

89
00:02:40,830 --> 00:02:41,830
but a lot has changed in a year.

90
00:02:42,750 --> 00:02:43,750
You've got threads now out,

91
00:02:44,579 --> 00:02:45,579
and I'd love to get into why
you did threads when you did,

92
00:02:49,236 --> 00:02:50,236
and the approach that you took

93
00:02:51,600 --> 00:02:52,600
and kind of when you made that decision.

94
00:02:53,010 --> 00:02:54,010
Cuz it seemed like it
happened pretty quickly.

95
00:02:56,040 --> 00:02:57,040
- Yeah, you know, I've
always thought that the,

96
00:03:02,956 --> 00:03:03,956
I think the aspiration of Twitter, right,

97
00:03:04,590 --> 00:03:05,590
to build this, you know,
text-based discussion

98
00:03:08,640 --> 00:03:09,640
should be a billion
person social app, right?

99
00:03:12,819 --> 00:03:13,819
I mean, there are certain
kind of fundamental

100
00:03:14,850 --> 00:03:15,850
social experiences that I look at them

101
00:03:17,460 --> 00:03:18,460
and I'm just like okay,
like if I were running that

102
00:03:19,500 --> 00:03:20,500
I could scale that to
reach a billion people.

103
00:03:22,876 --> 00:03:23,876
And that's one of the reasons
why over time we've done

104
00:03:25,560 --> 00:03:26,560
different acquisitions and
why we've considered them

105
00:03:27,660 --> 00:03:28,660
is when I've looked at different products,

106
00:03:28,980 --> 00:03:29,980
I'm like, okay, yeah, I
think that that is like

107
00:03:31,357 --> 00:03:32,357
something really good there

108
00:03:32,190 --> 00:03:33,190
we can get that to be a billion people.

109
00:03:34,532 --> 00:03:35,532
- You tried to buy Twitter
way back in the day, right?

110
00:03:35,910 --> 00:03:36,910
Like many, many years ago.

111
00:03:37,350 --> 00:03:38,350
- Yeah, I mean we had conversations.

112
00:03:39,270 --> 00:03:40,270
I think this was, gosh,
this was I think when Jack

113
00:03:43,290 --> 00:03:44,290
was leaving the first time.

114
00:03:45,079 --> 00:03:46,079
And look, I get it I mean
different entrepreneurs

115
00:03:47,790 --> 00:03:48,790
have different goals
for what they wanna do,

116
00:03:49,530 --> 00:03:50,530
and some people wanna run
their companies independently.

117
00:03:51,420 --> 00:03:52,420
And that's cool.

118
00:03:52,253 --> 00:03:53,253
I mean, it's good that there's sort of a

119
00:03:54,060 --> 00:03:55,060
diversity of different outcomes,

120
00:03:55,320 --> 00:03:56,320
but I guess Twitter was
sort of plodding along

121
00:03:58,560 --> 00:03:59,560
for a while before Elon came
and I think the rate of change

122
00:04:02,970 --> 00:04:03,970
in the product was pretty slow, right?

123
00:04:05,640 --> 00:04:06,640
So it just didn't seem like
they were on the trajectory

124
00:04:07,590 --> 00:04:08,590
that would maximize their potential.

125
00:04:09,780 --> 00:04:10,780
And then with Elon coming in
I think there was certainly

126
00:04:12,960 --> 00:04:13,960
an opportunity to change things up.

127
00:04:15,060 --> 00:04:16,060
And he has, right?

128
00:04:16,200 --> 00:04:17,200
I mean he's definitely a change agent.

129
00:04:18,240 --> 00:04:19,240
- Yeah.

130
00:04:19,073 --> 00:04:20,073
- Right?

131
00:04:20,056 --> 00:04:21,056
And I think it's still not
clear exactly what trajectory

132
00:04:23,100 --> 00:04:24,100
it's on, but I do think
he's been pretty polarizing.

133
00:04:26,220 --> 00:04:27,220
So I think that the chance
that it sort of reaches

134
00:04:28,890 --> 00:04:29,890
the full potential on the
trajectory that it's on

135
00:04:32,100 --> 00:04:33,100
is, I don't know, I guess
I'm probably less optimistic

136
00:04:36,720 --> 00:04:37,720
or just think there's less of a chance now

137
00:04:38,160 --> 00:04:39,160
than there was before.

138
00:04:39,150 --> 00:04:40,150
- [Interviewer] Hmm.

139
00:04:40,230 --> 00:04:41,230
- But I guess just
watching all this play out

140
00:04:41,940 --> 00:04:42,940
just kind of reminded me and
rekindled the sense that like

141
00:04:45,270 --> 00:04:46,270
someone should build a version of this

142
00:04:48,390 --> 00:04:49,390
that can be more ubiquitous.

143
00:04:51,060 --> 00:04:52,060
And I look at some of
the things around it.

144
00:04:53,101 --> 00:04:54,101
Like I think these days
people just want kind

145
00:04:57,379 --> 00:04:58,379
of a, well, let's put it this way,

146
00:05:01,230 --> 00:05:02,230
I think a lot of the
conversation around social media

147
00:05:05,700 --> 00:05:06,700
is around sort of like information
and the utility aspect,

148
00:05:10,710 --> 00:05:11,710
but I think an equally important part

149
00:05:12,960 --> 00:05:13,960
of designing any product is
how it makes you feel, right?

150
00:05:15,750 --> 00:05:16,750
What's the kind of emotional charge of it

151
00:05:18,450 --> 00:05:19,450
and how do you come
away from that feeling?

152
00:05:20,310 --> 00:05:21,310
And I think Instagram is generally kind of

153
00:05:24,570 --> 00:05:25,570
on the happier end of the spectrum.

154
00:05:26,160 --> 00:05:27,160
I think Facebook is sort of in the middle

155
00:05:27,750 --> 00:05:28,750
because it has happier moments,

156
00:05:29,010 --> 00:05:30,010
but then it also has sort of harder news

157
00:05:32,460 --> 00:05:33,460
and things like that,

158
00:05:33,390 --> 00:05:34,390
that I think tend to just be more critical

159
00:05:35,850 --> 00:05:36,850
and maybe make people see
some of the negative things

160
00:05:39,060 --> 00:05:40,060
that are going on in the world.

161
00:05:40,047 --> 00:05:41,047
And I think Twitter index is very strongly

162
00:05:42,557 --> 00:05:43,557
on just being quite negative and critical.

163
00:05:44,760 --> 00:05:45,760
- [Interviewer] Yeah.

164
00:05:46,637 --> 00:05:47,637
- It's not, you know, I
think that that's sort of

165
00:05:47,470 --> 00:05:48,470
the design, it's not
that the designers wanted

166
00:05:49,050 --> 00:05:50,050
to make people feel bad,

167
00:05:49,920 --> 00:05:50,920
I think they wanted to have like maximum

168
00:05:52,170 --> 00:05:53,170
kind of intense debate, right?

169
00:05:53,640 --> 00:05:54,640
Which and I think that
that sort of creates

170
00:05:55,470 --> 00:05:56,470
a certain emotional feeling and load.

171
00:05:58,290 --> 00:05:59,290
And I always just thought
you could create a discussion

172
00:06:02,010 --> 00:06:03,010
experience that wasn't
quite so negative or toxic.

173
00:06:09,192 --> 00:06:10,192
And I think in doing
so it would actually be

174
00:06:12,701 --> 00:06:13,701
more accessible to a lot of people.

175
00:06:13,534 --> 00:06:14,534
I think a lot of people just
don't want to use an app

176
00:06:16,461 --> 00:06:17,461
where they come away
feeling bad all the time.

177
00:06:19,740 --> 00:06:20,740
Right?

178
00:06:21,080 --> 00:06:22,080
I think that there's a certain
set of people will either

179
00:06:21,913 --> 00:06:22,913
tolerate that cuz it's their
job to get that access to

180
00:06:24,000 --> 00:06:25,000
information, or they're
just warriors in that way.

181
00:06:26,520 --> 00:06:27,520
- Yeah.

182
00:06:27,379 --> 00:06:28,379
- Where they wanna be a part of

183
00:06:28,397 --> 00:06:29,397
that kind of intellectual combat.

184
00:06:29,230 --> 00:06:30,230
- Yeah.

185
00:06:30,417 --> 00:06:31,417
- But I just, I don't think that that's

186
00:06:31,250 --> 00:06:32,250
the ubiquitous thing.

187
00:06:32,280 --> 00:06:33,280
- [Interviewer] Right.

188
00:06:33,837 --> 00:06:34,837
- I think the ubiquitous thing is like

189
00:06:35,443 --> 00:06:36,443
people they wanna get fresh information.

190
00:06:37,200 --> 00:06:38,200
I think that there's a
place for text-based, right?

191
00:06:39,360 --> 00:06:40,360
Even when the world is
moving towards richer

192
00:06:42,480 --> 00:06:43,480
and richer forms of
sharing and consumption.

193
00:06:45,090 --> 00:06:46,090
I think that text isn't going away.

194
00:06:47,040 --> 00:06:48,040
It's still gonna be a big thing,

195
00:06:48,540 --> 00:06:49,540
but I think how people
feel is really important.

196
00:06:51,060 --> 00:06:52,060
So that's been a big
part of how we've tried

197
00:06:53,940 --> 00:06:54,940
to emphasize and develop
threads and over time,

198
00:06:57,150 --> 00:06:58,150
if you want it to be ubiquitous,

199
00:06:59,120 --> 00:07:00,120
you obviously wanna be
welcome to everyone,

200
00:07:01,860 --> 00:07:02,860
but I think how you see the networks

201
00:07:03,990 --> 00:07:04,990
and the culture that you create there,

202
00:07:06,090 --> 00:07:07,090
I think ends up being pretty important

203
00:07:07,620 --> 00:07:08,620
for how they scale over time.

204
00:07:09,180 --> 00:07:10,180
Or with Facebook we started
with this real name culture

205
00:07:12,240 --> 00:07:13,240
and it was grounded to
your college email address

206
00:07:14,220 --> 00:07:15,220
and now it obviously hasn't
been grounded to your college

207
00:07:16,530 --> 00:07:17,530
email address for a very long time,

208
00:07:18,360 --> 00:07:19,360
but I think the kind of
real authentic identity

209
00:07:23,490 --> 00:07:24,490
aspect of Facebook has continued

210
00:07:25,200 --> 00:07:26,200
and continued to be an
important part of it.

211
00:07:27,300 --> 00:07:28,300
So I think how we set the
culture for threads early on

212
00:07:29,760 --> 00:07:30,760
in terms of being a more positive,

213
00:07:31,950 --> 00:07:32,950
friendly place for discussion

214
00:07:33,501 --> 00:07:34,501
will hopefully be one
of the defining elements

215
00:07:36,270 --> 00:07:37,270
for the next decade as we scale it out.

216
00:07:39,150 --> 00:07:40,150
We obviously have a lot of work to do,

217
00:07:40,659 --> 00:07:41,659
but I'd say it's off to
a quite a good start.

218
00:07:44,490 --> 00:07:45,490
I mean it's, and obviously
there was the huge spike

219
00:07:46,380 --> 00:07:47,380
and then not everyone who
tried it out originally

220
00:07:48,690 --> 00:07:49,690
is gonna stick around immediately.

221
00:07:50,370 --> 00:07:51,370
But I mean, the monthly
actives and weeklies,

222
00:07:52,440 --> 00:07:53,440
I mean I don't think we're
sharing stats on it yet-

223
00:07:54,810 --> 00:07:55,810
- You can if you'd like.

224
00:07:55,939 --> 00:07:56,939
- But it's good.

225
00:07:56,772 --> 00:07:57,772
No, I mean, I feel quite good about-

226
00:07:58,620 --> 00:07:59,620
- Really?

227
00:07:59,670 --> 00:08:00,670
- About that.

228
00:08:00,537 --> 00:08:01,537
- Cuz there's been the reporting out there

229
00:08:01,860 --> 00:08:02,860
that engagement kind of,

230
00:08:02,940 --> 00:08:03,940
which I think is natural
with any spike like that-

231
00:08:04,950 --> 00:08:05,950
- Yeah.

232
00:08:05,783 --> 00:08:06,783
- Your engagement's not gonna sustain,

233
00:08:06,960 --> 00:08:07,960
you guys kind of set I think
the original industry standard

234
00:08:10,020 --> 00:08:11,020
on engagement for these kind of products.

235
00:08:11,910 --> 00:08:12,910
So I assume you're
guiding towards a similar

236
00:08:14,400 --> 00:08:15,400
kind of metric that you-

237
00:08:15,480 --> 00:08:16,480
- Yeah, we just have this
playbook for how we do this.

238
00:08:18,378 --> 00:08:19,378
- Yeah.

239
00:08:19,377 --> 00:08:20,377
- And there's like phase
one is build a thing

240
00:08:21,330 --> 00:08:22,330
that kind of sparks some joy
and that people appreciate.

241
00:08:25,530 --> 00:08:26,530
Then from there you want
to get to something that is

242
00:08:28,200 --> 00:08:29,200
retentive so that way people
who have a good experience

243
00:08:31,980 --> 00:08:32,980
with a thing come back
and want to keep using it.

244
00:08:35,700 --> 00:08:36,700
And those two things are not,

245
00:08:37,170 --> 00:08:38,170
they're not always the same.

246
00:08:38,280 --> 00:08:39,280
There are a lot of things
that people think are awesome,

247
00:08:40,410 --> 00:08:41,410
but may not, you know,
always come back to,

248
00:08:42,450 --> 00:08:43,450
I mean, I think some of what
people are seeing now around

249
00:08:44,940 --> 00:08:45,940
like chatGPT is part of that.

250
00:08:46,350 --> 00:08:47,350
It's like, I mean this is
like this level of AI is,

251
00:08:49,530 --> 00:08:50,530
it's like a miracle, right?

252
00:08:50,670 --> 00:08:51,670
It's awesome.

253
00:08:51,503 --> 00:08:52,503
But I mean that doesn't mean that everyone

254
00:08:53,021 --> 00:08:54,021
is gonna have a use case every week.

255
00:08:54,301 --> 00:08:55,301
Right?

256
00:08:55,299 --> 00:08:56,299
- Right.

257
00:08:56,317 --> 00:08:57,317
- So I think that there's
first is like create the spark.

258
00:08:57,150 --> 00:08:58,150
Second is create the retention.

259
00:08:59,280 --> 00:09:00,280
Then once you have retention,

260
00:09:01,181 --> 00:09:02,181
then you can start encouraging
more people to join.

261
00:09:05,580 --> 00:09:06,580
But if people aren't
gonna be retained by it,

262
00:09:07,650 --> 00:09:08,650
why would you ask people to
go sign up for something?

263
00:09:09,510 --> 00:09:10,510
- [Interviewer] Sure.

264
00:09:10,848 --> 00:09:11,848
- Right?

265
00:09:11,681 --> 00:09:12,681
So kinda step one, spark,

266
00:09:13,140 --> 00:09:14,140
step two, retention,

267
00:09:14,827 --> 00:09:15,827
step three, growth and
scaling the community.

268
00:09:16,980 --> 00:09:17,980
And then only at that point is step four,

269
00:09:19,140 --> 00:09:20,140
which is monetization.

270
00:09:20,460 --> 00:09:21,460
And I mean we take a while
to go through all those.

271
00:09:24,330 --> 00:09:25,330
I mean we're really in some
sense only getting started

272
00:09:27,420 --> 00:09:28,420
on the monetization of
the messaging experiences.

273
00:09:30,870 --> 00:09:31,870
Like WhatsApp now with stuff
like business messaging-

274
00:09:33,690 --> 00:09:34,690
- Took a while.

275
00:09:34,523 --> 00:09:35,523
- Two billion people use the
product every day, right?

276
00:09:35,970 --> 00:09:36,970
- Right.

277
00:09:37,485 --> 00:09:38,485
- So I mean, we scaled it pretty far,

278
00:09:38,488 --> 00:09:39,488
but I think with our model
that that sort of works so.

279
00:09:42,821 --> 00:09:43,821
- To really, I don't, I mean,

280
00:09:44,880 --> 00:09:45,880
I know you're saying you
want to not necessarily,

281
00:09:47,400 --> 00:09:48,400
you are competing with Twitter,

282
00:09:48,630 --> 00:09:49,630
but you're trying to do it differently.

283
00:09:50,640 --> 00:09:51,640
To me as a Twitter addict for way too long

284
00:09:53,187 --> 00:09:54,187
and a very early threads user

285
00:09:55,187 --> 00:09:56,187
and I've been seeing
similar feedback from others

286
00:09:57,360 --> 00:09:58,360
like Adam Oseri has been
asking for feedback on threads.

287
00:09:59,820 --> 00:10:00,820
Is that it kind of still
lacks that real time feeling

288
00:10:03,000 --> 00:10:04,000
when you first open it of like,

289
00:10:04,230 --> 00:10:05,230
I'm gonna be getting
fresh cuz like what's what

290
00:10:06,990 --> 00:10:07,990
I go to Twitter for is news.

291
00:10:08,490 --> 00:10:09,490
- Yeah.

292
00:10:09,746 --> 00:10:10,746
- And I know you guys
aren't necessarily trying

293
00:10:10,579 --> 00:10:11,579
to emphasize news in this experience,

294
00:10:12,330 --> 00:10:13,330
which is a whole nother topic really,

295
00:10:13,650 --> 00:10:14,650
but like how do you get
that kind of Twitter

296
00:10:16,920 --> 00:10:17,920
like this is what's going
on right now feeling?

297
00:10:19,050 --> 00:10:20,050
I don't know if you agree.

298
00:10:20,467 --> 00:10:21,467
- Yeah no, I think it's a thing that we'll

299
00:10:21,464 --> 00:10:22,464
work on improving, but I mean,

300
00:10:24,685 --> 00:10:25,685
hard news content isn't
the only fresh content.

301
00:10:28,387 --> 00:10:29,387
- [Interviewer] Sure.

302
00:10:29,400 --> 00:10:30,400
- I think even within news
there's a whole spectrum

303
00:10:31,380 --> 00:10:32,380
between sort of hard critical news

304
00:10:33,838 --> 00:10:34,838
and like people
understanding what's going on

305
00:10:36,420 --> 00:10:37,420
with the sports that they follow.

306
00:10:37,962 --> 00:10:38,962
- [Interviewer] Right.

307
00:10:38,962 --> 00:10:39,962
- Or the celebrities that they
follow or things like that.

308
00:10:41,200 --> 00:10:42,200
And a lot of those things don't kind

309
00:10:42,420 --> 00:10:43,420
of leave people with the same,

310
00:10:43,590 --> 00:10:44,590
it's not like as cutting right,

311
00:10:45,030 --> 00:10:46,030
as a lot of the kind of hard news

312
00:10:46,440 --> 00:10:47,440
and especially the political discussion,

313
00:10:47,790 --> 00:10:48,790
I think is just so not, it's so polarized

314
00:10:51,300 --> 00:10:52,300
that I think it's hard to
come away from reading news

315
00:10:54,000 --> 00:10:55,000
about politics these days feeling good.

316
00:10:57,442 --> 00:10:58,442
- Yeah.

317
00:10:58,440 --> 00:10:59,440
- Right?

318
00:10:59,443 --> 00:11:00,443
And so I think that that's,

319
00:11:00,419 --> 00:11:01,419
but that doesn't go for everything.

320
00:11:01,421 --> 00:11:02,421
And part of this overall is
just how you tune the algorithm

321
00:11:05,160 --> 00:11:06,160
to basically encourage
either recency or quality,

322
00:11:10,980 --> 00:11:11,980
but less recency.

323
00:11:12,390 --> 00:11:13,390
So I'm not sure that we have
that balance exactly right yet

324
00:11:16,170 --> 00:11:17,170
it may be the case that
in a product like Threads

325
00:11:19,160 --> 00:11:20,160
where people may want to
see more recent content

326
00:11:24,360 --> 00:11:25,360
as opposed to something like
an Instagram or Facebook

327
00:11:27,330 --> 00:11:28,330
where it's more visual and
the balance might just be

328
00:11:30,120 --> 00:11:31,120
towards balancing towards
maybe a little more quality

329
00:11:33,270 --> 00:11:34,270
even if it's 12 hours ago
instead of two hours ago.

330
00:11:37,290 --> 00:11:38,290
So I think that this is the type of stuff

331
00:11:38,940 --> 00:11:39,940
that we need to tune and kind of optimize,

332
00:11:41,880 --> 00:11:42,880
but yeah, I think I agree with that point.

333
00:11:44,910 --> 00:11:45,910
- This hasn't happened yet with Threads,

334
00:11:46,650 --> 00:11:47,650
but you're eventually gonna
hook it into Activity Pub,

335
00:11:48,810 --> 00:11:49,810
which is this decentralized
social media protocol.

336
00:11:51,630 --> 00:11:52,630
- Yeah.

337
00:11:52,463 --> 00:11:53,463
- It's kind of complicated
in layman's terms,

338
00:11:54,480 --> 00:11:55,480
but essentially people
run their own servers.

339
00:11:56,070 --> 00:11:57,070
So instead of having a centralized company

340
00:11:57,690 --> 00:11:58,690
run the whole network, people
can run their own fiefdoms.

341
00:12:00,570 --> 00:12:01,570
It's federated.

342
00:12:01,403 --> 00:12:02,403
- [Mark Zuckerberg] Yeah.

343
00:12:02,883 --> 00:12:03,883
- So Threads will
eventually hook into this.

344
00:12:03,780 --> 00:12:04,780
- [Mark Zuckerberg] Yeah, yeah.

345
00:12:05,236 --> 00:12:06,236
- This is the first time
you've done anything I think

346
00:12:06,069 --> 00:12:07,069
really meaningful in the
decentralized social media space.

347
00:12:09,270 --> 00:12:10,270
- Yeah and we're building
it from the ground up.

348
00:12:10,650 --> 00:12:11,650
- Yeah.

349
00:12:11,960 --> 00:12:12,960
- I mean, I've always
believed in this stuff,

350
00:12:13,299 --> 00:12:14,299
I mean, a lot of this hasn't-

351
00:12:14,507 --> 00:12:15,507
- I mean, because you run
the largest centralized

352
00:12:17,160 --> 00:12:18,160
social media platform.

353
00:12:18,600 --> 00:12:19,600
- But I mean it didn't exist
when we got started, right?

354
00:12:20,277 --> 00:12:21,277
And I think the project of like,

355
00:12:22,170 --> 00:12:23,170
I mean I've had our team at various times

356
00:12:24,717 --> 00:12:25,717
do the thought experiment
of like, all right,

357
00:12:26,580 --> 00:12:27,580
what would it take to
move all of Facebook onto

358
00:12:30,179 --> 00:12:31,179
some kind of decentralized protocol?

359
00:12:33,160 --> 00:12:34,160
And it's like, that's
just not gonna happen.

360
00:12:34,560 --> 00:12:35,560
There's so much functionality
that is on Facebook

361
00:12:38,100 --> 00:12:39,100
that like it's way too
kind of complicated-

362
00:12:41,760 --> 00:12:42,760
- And well the technical data-

363
00:12:42,870 --> 00:12:43,870
- Support all the different things.

364
00:12:44,419 --> 00:12:45,419
- Yeah.

365
00:12:45,421 --> 00:12:46,421
- And it would just take so long

366
00:12:46,435 --> 00:12:47,435
and you'd not be innovating
during that time.

367
00:12:47,580 --> 00:12:48,580
And I think that there's value

368
00:12:49,110 --> 00:12:50,110
in being on one of these protocols,

369
00:12:51,210 --> 00:12:52,210
but it's not the only
way to deliver value.

370
00:12:53,640 --> 00:12:54,640
So the opportunity cost of
doing this massive transition

371
00:12:56,070 --> 00:12:57,070
is is kinda this massive thing,

372
00:12:57,930 --> 00:12:58,930
but when you're starting from scratch

373
00:12:59,400 --> 00:13:00,400
you can just design it
so it can work with that.

374
00:13:02,157 --> 00:13:03,157
And we wanted to do that with this

375
00:13:03,840 --> 00:13:04,840
because I thought that that
was one of the interesting

376
00:13:05,850 --> 00:13:06,850
things that's evolving around this

377
00:13:08,460 --> 00:13:09,460
kinda the Twitter competitive
space as a lot of the others.

378
00:13:11,037 --> 00:13:12,037
There is a real ecosystem around that

379
00:13:13,710 --> 00:13:14,710
and I think it's interesting.

380
00:13:15,600 --> 00:13:16,600
- So what does that mean for
a company like yours long term

381
00:13:18,570 --> 00:13:19,570
if people gravitate more towards

382
00:13:20,640 --> 00:13:21,640
these decentralized protocols over time?

383
00:13:23,250 --> 00:13:24,250
Where does a big centralized
player fit into that picture?

384
00:13:26,061 --> 00:13:27,061
- Well, I guess my view is that the more

385
00:13:32,670 --> 00:13:33,670
that there's interoperability
between different services

386
00:13:36,720 --> 00:13:37,720
and the more content can flow,

387
00:13:38,040 --> 00:13:39,040
the better all the services can be.

388
00:13:40,080 --> 00:13:41,080
And I guess I'm just confident enough

389
00:13:42,660 --> 00:13:43,660
that we can build the
best one of the services

390
00:13:44,757 --> 00:13:45,757
that I actually think that we'll benefit

391
00:13:48,750 --> 00:13:49,750
and we'll be able to build
better quality products

392
00:13:52,680 --> 00:13:53,680
by our products making sure
that we can have access

393
00:13:55,590 --> 00:13:56,590
to all of the different
content from wherever anyone

394
00:14:00,360 --> 00:14:01,360
is creating it.

395
00:14:01,193 --> 00:14:02,193
And like I get that not
everyone is gonna wanna use

396
00:14:03,981 --> 00:14:04,981
everything that we build.

397
00:14:05,460 --> 00:14:06,460
I mean, that's obviously the case.

398
00:14:06,930 --> 00:14:07,930
I mean, it's like, okay
we have 3 billion people

399
00:14:08,640 --> 00:14:09,640
using Facebook, but not everyone
wants to use one product.

400
00:14:12,210 --> 00:14:13,210
And I think the making it so
they can use an alternative,

401
00:14:15,240 --> 00:14:16,240
but can still interact
with people on the network

402
00:14:17,490 --> 00:14:18,490
will make it so that that
product also is more valuable.

403
00:14:20,190 --> 00:14:21,190
You can increase the
quality of the product

404
00:14:22,050 --> 00:14:23,050
by making it so that you
can give people access

405
00:14:24,840 --> 00:14:25,840
to all the content, even
if it wasn't created

406
00:14:27,270 --> 00:14:28,270
on that network itself.

407
00:14:28,590 --> 00:14:29,590
- Hmm.

408
00:14:29,661 --> 00:14:30,661
- So, I don't know.

409
00:14:30,680 --> 00:14:31,680
I mean, it's a bet.

410
00:14:31,677 --> 00:14:32,677
- Yeah.

411
00:14:32,701 --> 00:14:33,701
- I think it takes some confidence,

412
00:14:33,534 --> 00:14:34,534
but I don't know.

413
00:14:35,581 --> 00:14:36,581
I think it's both good
to give people choice.

414
00:14:38,019 --> 00:14:39,019
I dunno, there's kind of this
funny counterintuitive thing

415
00:14:41,219 --> 00:14:42,219
where I just don't think that people like

416
00:14:43,860 --> 00:14:44,860
feeling locked into a system.

417
00:14:45,420 --> 00:14:46,420
- Yeah.

418
00:14:46,600 --> 00:14:47,600
- So in a way I actually think
people will feel better about

419
00:14:49,020 --> 00:14:50,020
using our products if they know

420
00:14:50,880 --> 00:14:51,880
that they have the choice to leave.

421
00:14:52,380 --> 00:14:53,380
- Hmm.

422
00:14:53,501 --> 00:14:54,501
- Right?

423
00:14:54,499 --> 00:14:55,499
And if we make that super easy to happen

424
00:14:55,779 --> 00:14:56,779
and obviously there's a lot of competition

425
00:14:58,050 --> 00:14:59,050
and I mean we do download
your data on all our products

426
00:15:01,530 --> 00:15:02,530
and like people can do that today.

427
00:15:03,750 --> 00:15:04,750
But like the more that that's
designed in from scratch,

428
00:15:08,276 --> 00:15:09,276
I think it really just
gives creators for example,

429
00:15:12,270 --> 00:15:13,270
the sense that okay, like
I'm not, it's I have a-

430
00:15:15,757 --> 00:15:16,757
- Agency.

431
00:15:16,760 --> 00:15:17,760
- Yeah.

432
00:15:17,757 --> 00:15:18,757
- Yeah.

433
00:15:18,760 --> 00:15:19,760
- Yeah, so in a way that
actually makes people feel

434
00:15:19,620 --> 00:15:20,620
more confident investing in a system

435
00:15:21,480 --> 00:15:22,480
if they know that they have
freedom over how they operate.

436
00:15:23,236 --> 00:15:24,236
So, I don't know, I think
it's maybe for phase one

437
00:15:28,500 --> 00:15:29,500
of social networking it was
fine to like have these systems

438
00:15:32,370 --> 00:15:33,370
that people felt a
little more locked into,

439
00:15:33,990 --> 00:15:34,990
but I think for the mature
state of the ecosystem,

440
00:15:36,030 --> 00:15:37,030
I don't think that that's
gonna be where it goes.

441
00:15:37,710 --> 00:15:38,710
So I dunno, I'm pretty
optimistic about this.

442
00:15:39,510 --> 00:15:40,510
And then if we can build Threads on this

443
00:15:41,400 --> 00:15:42,400
then maybe we can over time

444
00:15:43,914 --> 00:15:44,914
as the standards get more built out,

445
00:15:46,710 --> 00:15:47,710
it's possible that we
can spread that to more

446
00:15:48,570 --> 00:15:49,570
of the stuff that we're doing.

447
00:15:51,299 --> 00:15:52,299
- Hmm.

448
00:15:52,317 --> 00:15:53,317
- We're certainly working
on Interop with messaging.

449
00:15:53,150 --> 00:15:54,150
- Yeah.

450
00:15:54,339 --> 00:15:55,339
- And I think that that's
been an important thing.

451
00:15:55,480 --> 00:15:56,480
The first step was kind
of getting Interop to work

452
00:15:57,210 --> 00:15:58,210
between our different messaging systems.

453
00:15:59,220 --> 00:16:00,220
- Right, so we can talk to each other.

454
00:16:00,840 --> 00:16:01,840
- Yeah and then the first
decision there was, okay,

455
00:16:03,210 --> 00:16:04,210
well WhatsApp, you know,

456
00:16:04,740 --> 00:16:05,740
we have this very strong
commitment to encryption.

457
00:16:06,930 --> 00:16:07,930
So if we're gonna Interop,

458
00:16:08,220 --> 00:16:09,220
then we're either gonna
make the others encrypted

459
00:16:09,900 --> 00:16:10,900
or we're gonna have to decrypt WhatsApp.

460
00:16:11,097 --> 00:16:12,097
And it's like, all right,

461
00:16:12,779 --> 00:16:13,779
well we're not gonna decrypt WhatsApp,

462
00:16:13,612 --> 00:16:14,612
so we're going to go down
the path of encrypting

463
00:16:15,390 --> 00:16:16,390
everything else, which we're
making good progress on,

464
00:16:16,980 --> 00:16:17,980
but that basically has just
meant completely rewriting

465
00:16:19,590 --> 00:16:20,590
Messenger and Instagram
direct from scratch.

466
00:16:22,020 --> 00:16:23,020
So you're basically going from a model

467
00:16:23,220 --> 00:16:24,220
where all the messages
are stored in the cloud.

468
00:16:25,650 --> 00:16:26,650
It's like you're completely
inverting the architecture

469
00:16:27,420 --> 00:16:28,420
where now all the messages
are stored locally

470
00:16:29,100 --> 00:16:30,100
and just the way that they're-

471
00:16:30,210 --> 00:16:31,210
- While the plane's in the air.

472
00:16:31,500 --> 00:16:32,500
- Yeah, yeah.

473
00:16:32,333 --> 00:16:33,333
So I mean, that's been sort of this like,

474
00:16:33,810 --> 00:16:34,810
heroic effort by just like
a hundred or more people

475
00:16:37,380 --> 00:16:38,380
over like a multi-year period.

476
00:16:39,450 --> 00:16:40,450
And we're basically getting to the point

477
00:16:40,980 --> 00:16:41,980
where it's starting to roll out now,

478
00:16:42,499 --> 00:16:43,499
but you know now that we're at the point

479
00:16:44,460 --> 00:16:45,460
where we can do encryption
across those apps,

480
00:16:47,800 --> 00:16:48,800
we can also start to support more Interop.

481
00:16:50,670 --> 00:16:51,670
Which I think is gonna be interesting too.

482
00:16:52,200 --> 00:16:53,200
- With other services like Meta

483
00:16:54,775 --> 00:16:55,775
that Meta doesn't own
other messaging apps.

484
00:16:56,131 --> 00:16:57,131
- Yeah well I mean, the
plan was always to start

485
00:16:57,395 --> 00:16:58,395
with Interop that between our services,

486
00:16:59,459 --> 00:17:00,459
and then to get to that, but yeah,

487
00:17:01,315 --> 00:17:02,315
we're starting to
experiment with that too.

488
00:17:04,140 --> 00:17:05,140
- I promise to stop bringing up Elon,

489
00:17:06,510 --> 00:17:07,510
but you and he were together
with Senator Chuck Schumer

490
00:17:09,300 --> 00:17:10,300
at the White House recently
for this big AI summit

491
00:17:12,420 --> 00:17:13,420
and a lot of it was closed.

492
00:17:14,850 --> 00:17:15,850
- Along with a lot of other people.

493
00:17:15,720 --> 00:17:16,720
- Along with a lot of other people.

494
00:17:16,650 --> 00:17:17,650
You guys were sitting at
opposite sides of the table.

495
00:17:18,180 --> 00:17:19,180
I thought that was an interesting choice.

496
00:17:20,099 --> 00:17:21,099
What was your takeaway from that

497
00:17:22,800 --> 00:17:23,800
and kind of where the
government is in the US

498
00:17:26,430 --> 00:17:27,430
on regulating AI?

499
00:17:27,870 --> 00:17:28,870
What do you think is gonna happen?

500
00:17:31,832 --> 00:17:32,832
- Well, I didn't really
know what to expect

501
00:17:32,665 --> 00:17:33,665
going into that conversation,

502
00:17:34,175 --> 00:17:35,175
but it was quite substantive.

503
00:17:37,650 --> 00:17:38,650
And I think it, you know, I
think we covered a lot more

504
00:17:40,920 --> 00:17:41,920
ground than I expected.

505
00:17:42,360 --> 00:17:43,360
And the thing that was
interesting, I mean,

506
00:17:43,800 --> 00:17:44,800
your question you asked about

507
00:17:45,390 --> 00:17:46,390
what does it say about
where the government is?

508
00:17:49,555 --> 00:17:50,555
But aside from Senator Schumer

509
00:17:51,510 --> 00:17:52,510
who basically moderated the discussion,

510
00:17:53,315 --> 00:17:54,315
it was really an opportunity for

511
00:17:55,656 --> 00:17:56,656
I guess for them to hear from the people

512
00:17:59,340 --> 00:18:00,340
in the tech industry, but
also folks in civil society.

513
00:18:03,219 --> 00:18:04,219
I mean, you had people running unions,

514
00:18:04,500 --> 00:18:05,500
you had people from Hollywood

515
00:18:06,900 --> 00:18:07,900
and representing all the
kind of creative industry

516
00:18:10,800 --> 00:18:11,800
and intellectual property.

517
00:18:12,600 --> 00:18:13,600
You had researchers, people
focused on AI safety.

518
00:18:16,770 --> 00:18:17,770
And one of the things
that I actually thought

519
00:18:18,858 --> 00:18:19,858
was the most interesting was the senators

520
00:18:22,221 --> 00:18:23,221
didn't really speak that much.

521
00:18:23,580 --> 00:18:24,580
There's sort of the meme that it's like,

522
00:18:26,017 --> 00:18:27,017
okay, like a lot of these politicians,

523
00:18:28,280 --> 00:18:29,280
they'll go to a place
where they'll get attention

524
00:18:30,690 --> 00:18:31,690
for themselves-

525
00:18:32,014 --> 00:18:33,014
- Yeah.

526
00:18:32,847 --> 00:18:33,847
- But this was a three hour event

527
00:18:34,350 --> 00:18:35,350
and I think there were like 40 senators

528
00:18:38,056 --> 00:18:39,056
like sitting and
listening and taking notes

529
00:18:41,437 --> 00:18:42,437
and not really participating
in the discussion,

530
00:18:43,080 --> 00:18:44,080
but just there I think to learn.

531
00:18:44,730 --> 00:18:45,730
And I thought that was super interesting.

532
00:18:46,230 --> 00:18:47,230
- Yeah.

533
00:18:47,859 --> 00:18:48,859
- Right?

534
00:18:48,856 --> 00:18:49,856
In a way that I think sort
of reflects pretty well

535
00:18:50,760 --> 00:18:51,760
on our system and the intellectual
curiosity of the people

536
00:18:53,340 --> 00:18:54,340
who are ultimately gonna be making

537
00:18:54,960 --> 00:18:55,960
those kind of legislative decisions.

538
00:18:57,597 --> 00:18:58,597
So I thought that was fascinating to see.

539
00:19:00,450 --> 00:19:01,450
- Yeah.

540
00:19:02,280 --> 00:19:03,280
- But no, I mean, I didn't come away,

541
00:19:03,939 --> 00:19:04,939
you know, apart from
seeing their heads nod

542
00:19:06,210 --> 00:19:07,210
when certain people made certain points,

543
00:19:08,577 --> 00:19:09,577
it wasn't a time for us
to really get their sense

544
00:19:12,060 --> 00:19:13,060
on where they are.

545
00:19:13,799 --> 00:19:14,799
I think it was more just they were hearing

546
00:19:15,937 --> 00:19:16,937
the discussion of the issues.

547
00:19:17,297 --> 00:19:18,297
- Have you seen some of the,

548
00:19:18,316 --> 00:19:19,316
I don't think it's necessarily
focused at you specifically,

549
00:19:19,920 --> 00:19:20,920
but the criticism that the
tech industry is performing

550
00:19:22,350 --> 00:19:23,350
regulatory capture right now with AI

551
00:19:24,540 --> 00:19:25,540
and is essentially trying
to take the drawbridge up

552
00:19:28,770 --> 00:19:29,770
with them here, are you
worried about that at all?

553
00:19:32,599 --> 00:19:33,599
- I have seen that concern

554
00:19:34,410 --> 00:19:35,410
and I'm somewhat worried about it myself.

555
00:19:39,158 --> 00:19:40,158
I mean, look, I think that
there are real concerns here.

556
00:19:42,977 --> 00:19:43,977
So I think that-

557
00:19:43,980 --> 00:19:44,980
- [Interviewer] Sure.

558
00:19:44,977 --> 00:19:45,977
- I think a lot of these folks

559
00:19:45,994 --> 00:19:46,994
are truly earnest in their concerns.

560
00:19:48,978 --> 00:19:49,978
And I think that there is valuable stuff

561
00:19:50,370 --> 00:19:51,370
for the government to do.

562
00:19:53,292 --> 00:19:54,292
I think both in terms of
protecting American citizens

563
00:19:57,060 --> 00:19:58,060
from harm and preserving
I think what is a natural

564
00:20:03,060 --> 00:20:04,060
competitive advantage
for the United States

565
00:20:05,730 --> 00:20:06,730
compared to other countries.

566
00:20:07,738 --> 00:20:08,738
I think this is just
gonna be a huge sector

567
00:20:09,840 --> 00:20:10,840
and it's gonna be
important for everything,

568
00:20:12,210 --> 00:20:13,210
not just in terms of the economy,

569
00:20:14,850 --> 00:20:15,850
but there's probably defense components

570
00:20:17,070 --> 00:20:18,070
and things like that.

571
00:20:18,560 --> 00:20:19,560
And I think the US having a
lead on that is important.

572
00:20:20,298 --> 00:20:21,298
- [Interviewer] Sure.

573
00:20:21,296 --> 00:20:22,296
- And I think having the
government think through, okay,

574
00:20:23,133 --> 00:20:24,133
well how do we wanna
leverage the fact that

575
00:20:24,871 --> 00:20:25,871
we have the leading work in
the world happening here,

576
00:20:26,400 --> 00:20:27,400
and how do we wanna kind of control that

577
00:20:28,530 --> 00:20:29,530
and to what restrictions
do we wanna put on that

578
00:20:31,624 --> 00:20:32,624
getting to other places?

579
00:20:32,628 --> 00:20:33,628
I think that that makes sense.

580
00:20:33,628 --> 00:20:34,628
So I know there are a
bunch of concerns there

581
00:20:35,610 --> 00:20:36,610
that I think are real.

582
00:20:37,213 --> 00:20:38,213
You know, one of the topics
that I've spent a lot of time

583
00:20:41,790 --> 00:20:42,790
thinking about is open source.

584
00:20:43,490 --> 00:20:44,490
- Yeah.

585
00:20:44,514 --> 00:20:45,514
- Right because we do a lot
of open source work at Meta,

586
00:20:46,710 --> 00:20:47,710
obviously not everything
we do is open source.

587
00:20:48,750 --> 00:20:49,750
There's a lot of closed systems too.

588
00:20:50,010 --> 00:20:51,010
I'm not like a zealot on this, right?

589
00:20:51,540 --> 00:20:52,540
But I think I'm probably, I
lean probably a little more

590
00:20:55,210 --> 00:20:56,210
pro open source than most
of the other big companies.

591
00:20:57,840 --> 00:20:58,840
And we believe that it's
generally positive to open source

592
00:21:04,110 --> 00:21:05,110
a lot of our infrastructure
for a few reasons.

593
00:21:08,846 --> 00:21:09,846
I mean, one is like, we don't
have a cloud business, right?

594
00:21:11,730 --> 00:21:12,730
So it's not like we're selling
access to the infrastructure.

595
00:21:14,690 --> 00:21:15,690
So giving it away is fine.

596
00:21:15,523 --> 00:21:16,523
And then when we do give it away,

597
00:21:17,580 --> 00:21:18,580
we generally benefit from
innovation from the ecosystem.

598
00:21:20,640 --> 00:21:21,640
And when other people adopt the stuff,

599
00:21:22,590 --> 00:21:23,590
it increases volume
and drives down prices.

600
00:21:24,690 --> 00:21:25,690
So if you look at stuff like-

601
00:21:26,413 --> 00:21:27,413
- Like PyTorch for example,

602
00:21:27,650 --> 00:21:28,650
- Well, when I was talking
about driving down prices,

603
00:21:28,770 --> 00:21:29,770
I was thinking about
stuff like open compute.

604
00:21:30,360 --> 00:21:31,360
- Yeah.

605
00:21:31,965 --> 00:21:32,965
- Where we open sourced our server designs

606
00:21:32,973 --> 00:21:33,973
and now the factories that are
making those kind of servers

607
00:21:36,090 --> 00:21:37,090
can generate way more of
them because other companies

608
00:21:39,600 --> 00:21:40,600
like Amazon and others are designing this

609
00:21:41,120 --> 00:21:42,120
or like ordering the same designs

610
00:21:43,325 --> 00:21:44,325
that drives down the price for everyone,

611
00:21:44,711 --> 00:21:45,711
which is good.

612
00:21:45,544 --> 00:21:46,544
PyTorch is great because
it basically makes it,

613
00:21:47,880 --> 00:21:48,880
that it's like the standard
across the industry

614
00:21:49,740 --> 00:21:50,740
as people develop with this,

615
00:21:51,180 --> 00:21:52,180
which means that more
libraries and modules

616
00:21:53,280 --> 00:21:54,280
are created for it,

617
00:21:54,750 --> 00:21:55,750
which just makes it better
and it makes it better for us

618
00:21:57,250 --> 00:21:58,250
to develop internally too.

619
00:21:58,083 --> 00:21:59,083
So, and then all that stuff is true

620
00:22:01,394 --> 00:22:02,394
and works well for open source.

621
00:22:03,010 --> 00:22:04,010
And also I think it's
pretty well established

622
00:22:05,474 --> 00:22:06,474
that open source software is
it's generally more secure

623
00:22:09,750 --> 00:22:10,750
and safer because it's just
more scrutinized, right?

624
00:22:12,751 --> 00:22:13,751
When more people can see stuff,

625
00:22:14,610 --> 00:22:15,610
every piece of software
has bugs and issues.

626
00:22:17,130 --> 00:22:18,130
But the more people who
can look at it, you know,

627
00:22:20,911 --> 00:22:21,911
the more you're gonna basically identify

628
00:22:23,234 --> 00:22:24,234
what those issues are and
have eyes on fixing them.

629
00:22:26,430 --> 00:22:27,430
And then also because
there's sort of a standard

630
00:22:28,973 --> 00:22:29,973
and that's deployed across the industry,

631
00:22:29,880 --> 00:22:30,880
those fixes get rolled out everywhere,

632
00:22:31,440 --> 00:22:32,440
which is a big advantage
for safety and security.

633
00:22:36,660 --> 00:22:37,660
And when I think about AI safety,

634
00:22:37,950 --> 00:22:38,950
I think one of the big issues,

635
00:22:39,570 --> 00:22:40,570
if there's like a single
super intelligence

636
00:22:41,550 --> 00:22:42,550
and it's closed and someone
figures out how to exploit it,

637
00:22:44,040 --> 00:22:45,040
then like everyone kinda gets
screwed at the same time.

638
00:22:47,572 --> 00:22:48,572
Whereas in an open source system,

639
00:22:48,480 --> 00:22:49,480
it's like okay, people find issues

640
00:22:50,310 --> 00:22:51,310
and just like your Mac or
whatever gets patched right,

641
00:22:52,830 --> 00:22:53,830
it's like people find the issues

642
00:22:54,792 --> 00:22:55,792
and then it just gets rolled
out across the industry.

643
00:22:55,917 --> 00:22:56,917
- Yeah.

644
00:22:56,914 --> 00:22:57,914
- So I think that's generally positive,

645
00:23:00,720 --> 00:23:01,720
but there's obviously this
whole debate where when you open

646
00:23:05,794 --> 00:23:06,794
source stuff, I mean, we
can build in safeguards,

647
00:23:07,680 --> 00:23:08,680
but if you open source something
you're not fundamentally

648
00:23:10,920 --> 00:23:11,920
gonna be able to prevent
bad guys from taking that

649
00:23:13,710 --> 00:23:14,710
and running with it too.

650
00:23:15,853 --> 00:23:16,853
Something that there is
sort of this debate around,

651
00:23:18,350 --> 00:23:19,350
okay, well what's the balance of,

652
00:23:19,613 --> 00:23:20,613
how capable do you want the
models that are open source?

653
00:23:24,055 --> 00:23:25,055
And I think that there
is a real debate there.

654
00:23:24,990 --> 00:23:25,990
I do sometimes get the
sense that some of the folks

655
00:23:27,180 --> 00:23:28,180
whose business model is
to basically sell access

656
00:23:31,037 --> 00:23:32,037
to the closed models
that they're developing.

657
00:23:32,400 --> 00:23:33,400
I do think that they have to be careful

658
00:23:33,900 --> 00:23:34,900
because they are also talking their book

659
00:23:35,640 --> 00:23:36,640
when they're talking about
dangers of open source.

660
00:23:37,980 --> 00:23:38,980
- [Interviewer] Right.

661
00:23:38,813 --> 00:23:39,813
- And I think that there
are dynamics like that

662
00:23:41,400 --> 00:23:42,400
that happen that I hear eithe-

663
00:23:45,511 --> 00:23:46,511
- [Interviewer] Sure.

664
00:23:46,514 --> 00:23:47,514
- You know, overtly or
sometimes behind closed doors,

665
00:23:48,514 --> 00:23:49,514
something will get back
to me that's like, oh,

666
00:23:49,769 --> 00:23:50,769
like this company was
kind of talking about

667
00:23:51,930 --> 00:23:52,930
why they're kind of against open source

668
00:23:54,210 --> 00:23:55,210
and it's like, yeah well
their whole business

669
00:23:56,648 --> 00:23:57,648
depends on selling access
to proprietary models.

670
00:23:58,110 --> 00:23:59,110
So I think you gotta
be careful about that.

671
00:23:59,700 --> 00:24:00,700
So I do think the regulatory
capture thing I think

672
00:24:03,240 --> 00:24:04,240
you need to be careful
about for things like that

673
00:24:05,040 --> 00:24:06,040
because I do think one of the
big benefits of open source

674
00:24:08,010 --> 00:24:09,010
is it also just decreases
the cost of adoption

675
00:24:13,110 --> 00:24:14,110
for small companies and
a lot of other folks.

676
00:24:17,394 --> 00:24:18,394
So I do think that's gonna be a big thing.

677
00:24:19,794 --> 00:24:20,794
- Which I think Llama
and the Llama Two release

678
00:24:21,210 --> 00:24:22,210
has been a big thing for
startups because it is so free.

679
00:24:25,009 --> 00:24:26,009
- Yeah.

680
00:24:26,013 --> 00:24:27,013
- Or just easy to use access.

681
00:24:27,010 --> 00:24:28,010
- Yeah.

682
00:24:27,870 --> 00:24:28,870
- And I guess I'm wondering, did you ever,

683
00:24:31,080 --> 00:24:32,080
was there ever debate internally about

684
00:24:34,350 --> 00:24:35,350
should we take the closed route you spent?

685
00:24:36,510 --> 00:24:37,510
- Oh yeah.

686
00:24:38,130 --> 00:24:39,130
- I mean you spent so much
money on all this AI research,

687
00:24:40,173 --> 00:24:41,173
you have one of the best
probably AI labs in the world

688
00:24:41,400 --> 00:24:42,400
I think it's safe to say.

689
00:24:43,111 --> 00:24:44,111
You have huge distribution.

690
00:24:45,120 --> 00:24:46,120
Why not keep it all to yourself?

691
00:24:48,311 --> 00:24:49,311
You could have done that.

692
00:24:49,346 --> 00:24:50,346
- Yeah, you know, the biggest arguments

693
00:24:51,660 --> 00:24:52,660
in favor of of keeping it closed

694
00:24:54,540 --> 00:24:55,540
were generally not proprietary advantage.

695
00:24:58,514 --> 00:24:59,514
- Or competitive advantage?

696
00:25:00,720 --> 00:25:01,720
- No, no, it wasn't competitive advantage.

697
00:25:02,911 --> 00:25:03,911
I think it's, the two,

698
00:25:06,631 --> 00:25:07,631
and there was a fairly
intense debate around this.

699
00:25:08,190 --> 00:25:09,190
- And did you have to be dissuade?

700
00:25:10,889 --> 00:25:11,889
Did you know we have to
have it open and you-

701
00:25:12,270 --> 00:25:13,270
- My bias was that I
thought it should be open.

702
00:25:14,520 --> 00:25:15,520
- Okay.

703
00:25:16,093 --> 00:25:17,093
- But I thought that there were
novel arguments on the risks

704
00:25:19,874 --> 00:25:20,874
and I wanted to make sure
we heard them all out

705
00:25:21,270 --> 00:25:22,270
and we did a very rigorous process.

706
00:25:22,574 --> 00:25:23,574
And my guess is that when
we're training the next version

707
00:25:25,351 --> 00:25:26,351
of Llama now and I think we'll
probably have the same set of

708
00:25:27,989 --> 00:25:28,989
debates around that and
how we should release it.

709
00:25:30,007 --> 00:25:31,007
And again, I sort of like
lean towards wanting to do it

710
00:25:32,006 --> 00:25:33,006
open source, but I think we
need to do all the red teaming

711
00:25:33,720 --> 00:25:34,720
and understand the risks
and before making a call.

712
00:25:37,910 --> 00:25:38,910
But I think the two big
arguments that people

713
00:25:41,646 --> 00:25:42,646
had against making Llama
Two open were one is,

714
00:25:48,231 --> 00:25:49,231
is just that it takes a
lot of time to prepare

715
00:25:49,806 --> 00:25:50,806
something to be open.

716
00:25:50,807 --> 00:25:51,807
So, I mean, our main business

717
00:25:52,387 --> 00:25:53,387
is basically building
consumer products, right?

718
00:25:56,130 --> 00:25:57,130
And that's where we're
launching at Connect.

719
00:25:57,367 --> 00:25:58,367
- Right.

720
00:25:58,367 --> 00:25:59,367
- Llama Two is not a consumer product,

721
00:25:59,366 --> 00:26:00,366
it's sort of the engine or infrastructure

722
00:26:00,960 --> 00:26:01,960
that powers a bunch of that stuff,

723
00:26:02,760 --> 00:26:03,760
but there was this sort of this argument,

724
00:26:04,560 --> 00:26:05,560
especially after we sort
of did this partial release

725
00:26:08,791 --> 00:26:09,791
of Llama One and there was
like a lot of stir around that

726
00:26:12,600 --> 00:26:13,600
and then people had a bunch
of feedback and were wondering

727
00:26:15,650 --> 00:26:16,650
when we were incorporate that
feedback, which kinda like,

728
00:26:18,551 --> 00:26:19,551
okay, well if we release Llama Two

729
00:26:19,384 --> 00:26:20,384
is that gonna distract
us from our real job,

730
00:26:23,650 --> 00:26:24,650
which is building the best
consumer products that we can.

731
00:26:25,911 --> 00:26:26,911
So that was one debate.

732
00:26:27,351 --> 00:26:28,351
I think we sort of got comfortable

733
00:26:29,550 --> 00:26:30,550
with that relatively quickly.

734
00:26:30,990 --> 00:26:31,990
And then the much bigger debate was around

735
00:26:33,127 --> 00:26:34,127
the risk and safety.

736
00:26:35,549 --> 00:26:36,549
- [Interviewer] Hmm.

737
00:26:36,567 --> 00:26:37,567
- And I think it's sort
of like how do you,

738
00:26:38,327 --> 00:26:39,327
what is the framework for
how you measure kind of

739
00:26:41,762 --> 00:26:42,762
what harm can be done and
how do you compare that

740
00:26:44,887 --> 00:26:45,887
to other things, you know?

741
00:26:47,187 --> 00:26:48,187
So for example someone
made this point recently,

742
00:26:50,670 --> 00:26:51,670
and this was actually at the Senate event.

743
00:26:53,527 --> 00:26:54,527
I mean, someone made
this point that's like,

744
00:26:54,930 --> 00:26:55,930
okay, well you know, we took Llama Two

745
00:26:57,900 --> 00:26:58,900
and our engineers in just
several days we're able

746
00:27:02,509 --> 00:27:03,509
to take away the safeguards
and ask it a question

747
00:27:04,650 --> 00:27:05,650
to can you produce anthrax?

748
00:27:07,191 --> 00:27:08,191
And it answered and it's on its
face that sounds really bad.

749
00:27:11,430 --> 00:27:12,430
Right?

750
00:27:13,090 --> 00:27:14,090
That's obviously an issue
that you can strip off the

751
00:27:14,103 --> 00:27:15,103
safeguards until you think
about the fact that you can

752
00:27:16,230 --> 00:27:17,230
actually just Google how to make anthrax

753
00:27:17,880 --> 00:27:18,880
and it shows up on the
first page of the results

754
00:27:19,560 --> 00:27:20,560
in five seconds, right?

755
00:27:20,790 --> 00:27:21,790
So I do think that there's a
question when you're thinking

756
00:27:23,790 --> 00:27:24,790
through these things
about what is the actual

757
00:27:25,740 --> 00:27:26,740
incremental risk that is created

758
00:27:28,110 --> 00:27:29,110
by having these different technologies.

759
00:27:30,567 --> 00:27:31,567
And maybe like, I think
a lot of this stuff,

760
00:27:34,126 --> 00:27:35,126
we've seen this in like
protecting social media as well.

761
00:27:37,200 --> 00:27:38,200
If you have like Russia or
some country trying to create

762
00:27:41,190 --> 00:27:42,190
a network of bots or inauthentic behavior,

763
00:27:45,469 --> 00:27:46,469
it's not that you're ever
gonna stop them from doing it,

764
00:27:46,500 --> 00:27:47,500
it's sort of an economics problem.

765
00:27:49,108 --> 00:27:50,108
You wanna make it expensive
enough for them to do that,

766
00:27:51,183 --> 00:27:52,183
that it is no longer their best strategy

767
00:27:53,280 --> 00:27:54,280
because it's cheaper for them to go try

768
00:27:55,726 --> 00:27:56,726
to exploit someone else
or something else, right?

769
00:27:57,330 --> 00:27:58,330
And I think the same is true here, right?

770
00:27:58,920 --> 00:27:59,920
So for the risk on this,
you wanna make it so that

771
00:28:03,189 --> 00:28:04,189
it's sufficiently expensive
that it takes engineers

772
00:28:05,460 --> 00:28:06,460
several days to dismantle
whatever safeguards we built in

773
00:28:08,940 --> 00:28:09,940
instead of just Googling it, right?

774
00:28:10,560 --> 00:28:11,560
So, so-

775
00:28:12,270 --> 00:28:13,270
- [Interviewer] You feel
generally good directionally

776
00:28:13,890 --> 00:28:14,890
with the safety work on the-

777
00:28:16,087 --> 00:28:17,087
- For Llama Two I mean I
think that we did leading work

778
00:28:19,329 --> 00:28:20,329
on that, I think the white
paper around Llama Two

779
00:28:20,610 --> 00:28:21,610
where we basically outlined
all the different metrics

780
00:28:23,190 --> 00:28:24,190
and all the different things that we did.

781
00:28:25,991 --> 00:28:26,991
- [Interviewer] Yeah.

782
00:28:26,989 --> 00:28:27,989
- And we did internal red
teaming and external red teaming.

783
00:28:27,822 --> 00:28:28,822
And we've got a bunch of feedback on it.

784
00:28:29,550 --> 00:28:30,550
So because we went into this knowing

785
00:28:32,606 --> 00:28:33,606
that nothing is gonna be foolproof, right?

786
00:28:34,231 --> 00:28:35,231
It like said we're gonna,

787
00:28:35,430 --> 00:28:36,430
some bad actor is going to be able

788
00:28:38,269 --> 00:28:39,269
to find some way to exploit it.

789
00:28:39,407 --> 00:28:40,407
We really knew that we needed

790
00:28:40,240 --> 00:28:41,240
to create a pretty high bar on that.

791
00:28:41,490 --> 00:28:42,490
So yeah, no, I felt good
about that for Llama Two,

792
00:28:44,504 --> 00:28:45,504
but it was a very rigorous process.

793
00:28:46,231 --> 00:28:47,231
- And you guys have now
announced the Meta AI agent,

794
00:28:47,640 --> 00:28:48,640
which is your proprietary,
I'm sure it's using Llama

795
00:28:51,131 --> 00:28:52,131
technology, but it's a closed model.

796
00:28:53,447 --> 00:28:54,447
You're not really disclosing
a lot about the model

797
00:28:55,751 --> 00:28:56,751
and its weights and all that.

798
00:28:56,610 --> 00:28:57,610
- Yeah.

799
00:28:58,048 --> 00:28:59,048
- But this is the new agent that people

800
00:28:59,271 --> 00:29:00,271
are gonna be seeing in the apps.

801
00:29:00,570 --> 00:29:01,570
- Yeah, yeah., so I mean,
at Connect we announced

802
00:29:02,950 --> 00:29:03,950
a bunch of different things on this.

803
00:29:05,460 --> 00:29:06,460
So Meta AI and the other
AI's that we released

804
00:29:09,568 --> 00:29:10,568
they're based on Llama Two, right?

805
00:29:12,667 --> 00:29:13,667
So it's not like exactly the same thing

806
00:29:14,670 --> 00:29:15,670
that we open sourced because
we used that as the foundation

807
00:29:18,900 --> 00:29:19,900
and then we kind of like
built on top of that

808
00:29:21,611 --> 00:29:22,611
to build the consumer products.

809
00:29:24,510 --> 00:29:25,510
But yeah, there were
a few different things

810
00:29:26,160 --> 00:29:27,160
that we announced.

811
00:29:27,467 --> 00:29:28,467
So like Meta AI-

812
00:29:28,300 --> 00:29:29,300
- I feel like that
part, the AI to me feels

813
00:29:30,990 --> 00:29:31,990
like the biggest deal in the near term

814
00:29:32,520 --> 00:29:33,520
cuz a lot of people
are gonna be seeing it.

815
00:29:34,907 --> 00:29:35,907
It may be the first time,

816
00:29:36,150 --> 00:29:37,150
even with all the coverage of like GPT,

817
00:29:38,810 --> 00:29:39,810
it may be the first time that
a lot of people experience

818
00:29:40,830 --> 00:29:41,830
a chat bot like this actually.

819
00:29:42,840 --> 00:29:43,840
- Yeah I mean, I'm really curious.

820
00:29:45,467 --> 00:29:46,467
- Which is different because-

821
00:29:46,470 --> 00:29:47,470
- Yeah, I'm very curious
to see how this stuff gets-

822
00:29:48,107 --> 00:29:49,107
- I used it for a little
bit and it has web,

823
00:29:49,740 --> 00:29:50,740
you know it can pull in web
results so it's got recency,

824
00:29:52,140 --> 00:29:53,140
which is nice.

825
00:29:53,610 --> 00:29:54,610
It wouldn't give me
advice on how to break up

826
00:29:56,987 --> 00:29:57,987
with my girlfriend, but it-

827
00:29:58,427 --> 00:29:59,427
- [Mark Zuckerberg] It wouldn't?

828
00:29:59,260 --> 00:30:00,260
- I don't, I know of a girlfriend.

829
00:30:00,625 --> 00:30:01,625
I was just trying to see
like what I'm married,

830
00:30:03,451 --> 00:30:04,451
but I was trying to see what
it won't and will answer.

831
00:30:07,590 --> 00:30:08,590
It seems relatively safe.

832
00:30:09,300 --> 00:30:10,300
- It seems like the type
of thing that it should be

833
00:30:10,560 --> 00:30:11,560
fine giving you advice.

834
00:30:12,907 --> 00:30:13,907
- Well, I'm just telling you.

835
00:30:14,968 --> 00:30:15,968
But what do you imagine
people using this for?

836
00:30:15,870 --> 00:30:16,870
Cuz it's got that search engine component,

837
00:30:18,030 --> 00:30:19,030
but it can do a lot of things.

838
00:30:19,680 --> 00:30:20,680
Is this a pure GPT, Chat GPT competitor

839
00:30:23,580 --> 00:30:24,580
in almost every way in your mind?

840
00:30:25,967 --> 00:30:26,967
Or how do you think about it?

841
00:30:27,211 --> 00:30:28,211
- Well, I think that there's a bunch

842
00:30:28,249 --> 00:30:29,249
of different spaces here

843
00:30:29,510 --> 00:30:30,510
so that I think people are gonna wanna

844
00:30:31,107 --> 00:30:32,107
interact with AI around.

845
00:30:32,330 --> 00:30:33,330
Take a step back.

846
00:30:33,447 --> 00:30:34,447
I think that the vision
for a bunch of folks

847
00:30:34,590 --> 00:30:35,590
in the industry when I look
at like open AI or Google

848
00:30:38,368 --> 00:30:39,368
is the sense that there's
gonna be like one big

849
00:30:40,107 --> 00:30:41,107
super intelligence and they want to be it.

850
00:30:42,090 --> 00:30:43,090
- [Interviewer] Yeah.

851
00:30:42,923 --> 00:30:43,923
- I just don't think that
that's the best future.

852
00:30:45,687 --> 00:30:46,687
I think the way that people
tend to process the world

853
00:30:49,804 --> 00:30:50,804
is we don't have one person
that we go to for everything.

854
00:30:51,030 --> 00:30:52,030
We don't have one app that
we go to for everything.

855
00:30:54,091 --> 00:30:55,091
I don't think that we want one AI.

856
00:30:54,924 --> 00:30:55,924
- It's overwhelming.

857
00:30:55,757 --> 00:30:56,757
I find this with the current chat bots,

858
00:30:57,540 --> 00:30:58,540
I'm like I feel like it can do so much

859
00:30:59,550 --> 00:31:00,550
that I'm not actually sure what to ask it.

860
00:31:01,620 --> 00:31:02,620
- Yeah, so I mean our view
is that there are actually

861
00:31:06,491 --> 00:31:07,491
gonna be a lot of these, right?

862
00:31:07,808 --> 00:31:08,808
That people talk to you
for different things

863
00:31:08,827 --> 00:31:09,827
and one thought experiment that I did

864
00:31:10,525 --> 00:31:11,525
to sort of prove to myself
that this would be the case

865
00:31:12,930 --> 00:31:13,930
is like, all right, let's
say you're a small business

866
00:31:15,390 --> 00:31:16,390
and you want to have an AI
that can help you interface

867
00:31:18,630 --> 00:31:19,630
with customers to do
sales and and support.

868
00:31:22,827 --> 00:31:23,827
You wanna be pretty confident that your AI

869
00:31:24,300 --> 00:31:25,300
isn't gonna be promoting
your competitor's products.

870
00:31:27,248 --> 00:31:28,248
- [Interviewer] Sure.

871
00:31:28,267 --> 00:31:29,267
- Right?

872
00:31:29,270 --> 00:31:30,270
So you want it to be yours,

873
00:31:30,267 --> 00:31:31,267
you want it to be aligned with you.

874
00:31:31,270 --> 00:31:32,270
So you're gonna want a separate agent

875
00:31:32,310 --> 00:31:33,310
then your competitor's agent.

876
00:31:34,530 --> 00:31:35,530
So then you get to this point where, okay,

877
00:31:37,408 --> 00:31:38,408
well there are gonna
be a hundred million AI

878
00:31:39,990 --> 00:31:40,990
just helping businesses sell things.

879
00:31:41,580 --> 00:31:42,580
Then you get the creator version of that

880
00:31:44,187 --> 00:31:45,187
where like every creator
I think is gonna want

881
00:31:48,070 --> 00:31:49,070
an AI assistant or something
that can help them build

882
00:31:52,008 --> 00:31:53,008
their community and people
are gonna really want

883
00:31:54,571 --> 00:31:55,571
to interact with it's like
there's just way more demand

884
00:31:56,310 --> 00:31:57,310
to interact with creators and celebrities.

885
00:31:58,647 --> 00:31:59,647
- There's only one Kylie Jenner.

886
00:31:59,480 --> 00:32:00,480
- Yeah.

887
00:32:00,651 --> 00:32:01,651
- And like you can you-

888
00:32:01,648 --> 00:32:02,648
- So it's this, I mean there's
I think a huge need here.

889
00:32:03,510 --> 00:32:04,510
People wanna interact with Kylie,

890
00:32:05,280 --> 00:32:06,280
Kylie wants to cultivate her community,

891
00:32:08,070 --> 00:32:09,070
but there are only so many hours in a day.

892
00:32:10,171 --> 00:32:11,171
- Right.

893
00:32:11,168 --> 00:32:12,168
- So creating an AI that's
sort of an assistant for her

894
00:32:13,380 --> 00:32:14,380
or it'll be clear to people
that they're not interacting

895
00:32:16,380 --> 00:32:17,380
with like the physical Kylie Jenner.

896
00:32:19,510 --> 00:32:20,510
It would be kinda an AI version

897
00:32:21,180 --> 00:32:22,180
and that'll help the creators

898
00:32:23,550 --> 00:32:24,550
and I think it'll be fun for consumers.

899
00:32:25,325 --> 00:32:26,325
That one's actually really hard.

900
00:32:26,940 --> 00:32:27,940
There's a lot of work on,

901
00:32:28,200 --> 00:32:29,200
I guess you can call it
brand safety concerns

902
00:32:30,060 --> 00:32:31,060
cuz like you really wanna
make sure that these AI's

903
00:32:36,548 --> 00:32:37,548
like reflect the
personality of the creator

904
00:32:39,826 --> 00:32:40,826
and don't talk about
things that the creator

905
00:32:41,040 --> 00:32:42,040
doesn't want to get into or-

906
00:32:42,690 --> 00:32:43,690
- [Interviewer] Right.

907
00:32:44,048 --> 00:32:45,048
- Don't say things that
are gonna be problematic

908
00:32:44,881 --> 00:32:45,881
for the creator and
they're endorsing fields.

909
00:32:46,440 --> 00:32:47,440
- I feel like having input in all of this,

910
00:32:47,970 --> 00:32:48,970
they should be able to
like say I don't want this.

911
00:32:50,528 --> 00:32:51,528
- Oh yeah, yeah, but I think
in some ways the technology

912
00:32:52,110 --> 00:32:53,110
doesn't even exist yet
to make it that trained.

913
00:32:55,712 --> 00:32:56,712
I mean this isn't code in
the deterministic sense.

914
00:32:58,683 --> 00:32:59,683
It's like a model that you
need to be able to train

915
00:33:00,600 --> 00:33:01,600
it to stay within certain bounds.

916
00:33:02,190 --> 00:33:03,190
And I think a lot of that
is still getting developed.

917
00:33:05,227 --> 00:33:06,227
- So that's more next year.

918
00:33:06,060 --> 00:33:07,060
- Yeah.

919
00:33:07,248 --> 00:33:08,248
So anyhow, there's
businesses, there's creators.

920
00:33:10,260 --> 00:33:11,260
That stuff is fun where the business stuff

921
00:33:12,060 --> 00:33:13,060
is I think more useful.

922
00:33:13,410 --> 00:33:14,410
And then I think that
there's a bunch of stuff

923
00:33:14,940 --> 00:33:15,940
that's just interesting
kind of consumer use cases.

924
00:33:19,707 --> 00:33:20,707
So there's more of like the utility,

925
00:33:20,850 --> 00:33:21,850
which is what Meta AI is

926
00:33:23,286 --> 00:33:24,286
like answer any question,

927
00:33:24,288 --> 00:33:25,288
you'll be able to use it to
help navigate your Quest3

928
00:33:28,500 --> 00:33:29,500
and the new Ray-Ban glasses
that we're shipping,

929
00:33:33,147 --> 00:33:34,147
which we should get to that in a second.

930
00:33:34,011 --> 00:33:35,011
- Yeah.

931
00:33:35,030 --> 00:33:36,030
- Because that'll be pretty wild,

932
00:33:36,032 --> 00:33:37,032
is having Meta AI that you
can just talk to all day long

933
00:33:38,374 --> 00:33:39,374
on your glasses.

934
00:33:41,072 --> 00:33:42,072
So yeah, I think that'll
be pretty powerful.

935
00:33:42,212 --> 00:33:43,212
But then there are also
gonna be all these other

936
00:33:44,430 --> 00:33:45,430
new characters that are getting created.

937
00:33:47,531 --> 00:33:48,531
- [Interviewer] Yeah.

938
00:33:48,534 --> 00:33:49,534
- Which is somewhat of an
easier question to start with

939
00:33:50,280 --> 00:33:51,280
than having AI that are kind
of acting as a real person

940
00:33:55,550 --> 00:33:56,550
because there aren't as many
kind of brand safety concerns

941
00:33:57,390 --> 00:33:58,390
around that, but they
could still be pretty fun.

942
00:33:59,310 --> 00:34:00,310
So we're experimenting with
a bunch of different AI's

943
00:34:04,440 --> 00:34:05,440
for different interests that people have,

944
00:34:06,180 --> 00:34:07,180
whether it's interested in
different kinds of sports

945
00:34:08,340 --> 00:34:09,340
or fashion or-

946
00:34:09,630 --> 00:34:10,630
- The one I tried was a travel agent type.

947
00:34:11,400 --> 00:34:12,400
- Yeah travel, yeah.

948
00:34:12,971 --> 00:34:13,971
There's some that are more
around giving people advice.

949
00:34:16,260 --> 00:34:17,260
There's like life coach
and like an aunt, right?

950
00:34:20,310 --> 00:34:21,310
And then there's some that
are more gamey, right?

951
00:34:23,392 --> 00:34:24,392
So there's like Snoop Dog is
playing the Dungeon Master

952
00:34:27,830 --> 00:34:28,830
and that I think is there's
like a few that are just

953
00:34:31,740 --> 00:34:32,740
these text-based adventure games

954
00:34:33,450 --> 00:34:34,450
and the ability to just
drop that into a Thread

955
00:34:35,670 --> 00:34:36,670
and play a text-based game I
think is gonna be super fun.

956
00:34:38,550 --> 00:34:39,550
So I think like part of
this is we want to create

957
00:34:42,390 --> 00:34:43,390
a diversity of different
experiences to see what resonates

958
00:34:44,940 --> 00:34:45,940
and what we want to go deeper on.

959
00:34:48,091 --> 00:34:49,091
This is sort of the first
step towards building

960
00:34:50,190 --> 00:34:51,190
this AI studio that we're working on

961
00:34:52,470 --> 00:34:53,470
that will make it so anyone
can build their own AI's.

962
00:34:57,171 --> 00:34:58,171
Sort of just like you create your own UGC,

963
00:34:59,152 --> 00:35:00,152
your own content on
across social networks.

964
00:35:01,712 --> 00:35:02,712
So you should be able to create
your own AI and publish it.

965
00:35:05,632 --> 00:35:06,632
And I think it's gonna be really wild.

966
00:35:09,350 --> 00:35:10,350
- I do agree it's gonna be wild.

967
00:35:10,183 --> 00:35:11,183
There's a bit of an easiness to it for me

968
00:35:12,694 --> 00:35:13,694
of just the idea that we as a society

969
00:35:15,480 --> 00:35:16,480
are going to be increasingly
having relationships with AI.

970
00:35:22,512 --> 00:35:23,512
I mean there's stories
about like character AI,

971
00:35:25,190 --> 00:35:26,190
which has a similar kind
of library of personas

972
00:35:27,090 --> 00:35:28,090
you can interact with and people literally

973
00:35:29,851 --> 00:35:30,851
like falling in love with
some of these chatbots.

974
00:35:32,668 --> 00:35:33,668
I mean, what do you think
about that phenomenon?

975
00:35:35,030 --> 00:35:36,030
Is it just inevitable with
where the tech is going?

976
00:35:39,472 --> 00:35:40,472
- That's not where we're starting.

977
00:35:41,608 --> 00:35:42,608
So I think that there's a lot of use cases

978
00:35:42,510 --> 00:35:43,510
that are just a lot more
clear than that right,

979
00:35:46,934 --> 00:35:47,934
in terms of someone who can
help you make workouts, right?

980
00:35:50,460 --> 00:35:51,460
Someone who can help you with cooking-

981
00:35:51,900 --> 00:35:52,900
- More utility.

982
00:35:53,590 --> 00:35:54,590
- Help you figure out travel, right?

983
00:35:54,630 --> 00:35:55,630
Or even like the game type stuff.

984
00:35:56,727 --> 00:35:57,727
I think that there's more kind of,

985
00:35:59,414 --> 00:36:00,414
I think that a bunch of
these things can help you

986
00:36:02,580 --> 00:36:03,580
in your interactions with people.

987
00:36:05,254 --> 00:36:06,254
And I think that that's
more our natural space.

988
00:36:08,672 --> 00:36:09,672
One of the things that
I think is gonna be,

989
00:36:09,789 --> 00:36:10,789
that we can do that's
harder for others to do

990
00:36:13,290 --> 00:36:14,290
is the ability to make
it so you can drop these

991
00:36:15,660 --> 00:36:16,660
into group chats, right?

992
00:36:17,734 --> 00:36:18,734
So we're starting with Meta AI,

993
00:36:18,770 --> 00:36:19,770
you can just invoke it in any Thread.

994
00:36:20,432 --> 00:36:21,432
Like I could be having a
one-on-one thread with you

995
00:36:22,070 --> 00:36:23,070
and I could just ask meta AI something.

996
00:36:23,531 --> 00:36:24,531
I can do that in a group chat thread.

997
00:36:25,331 --> 00:36:26,331
So I think that that's
gonna be really fun, right?

998
00:36:29,414 --> 00:36:30,414
It's just having these
kind of fun personalities

999
00:36:33,780 --> 00:36:34,780
in these threads I think will create

1000
00:36:36,971 --> 00:36:37,971
sort of an interesting dynamic.

1001
00:36:38,886 --> 00:36:39,886
I think especially
around image generation.

1002
00:36:40,249 --> 00:36:41,249
And we haven't talked about that as much.

1003
00:36:41,550 --> 00:36:42,550
- I used that it was pretty
impressive and it was fast.

1004
00:36:43,620 --> 00:36:44,620
- I think the team has
made awesome progress.

1005
00:36:45,720 --> 00:36:46,720
I mean we're at good
photorealistic quality,

1006
00:36:48,849 --> 00:36:49,849
but yeah like you said, I think that-

1007
00:36:51,090 --> 00:36:52,090
- For people who haven't used it yet,

1008
00:36:53,051 --> 00:36:54,051
you just type into the bot
what you want the image to be

1009
00:36:55,334 --> 00:36:56,334
and it'll just make it.

1010
00:36:56,331 --> 00:36:57,331
- Yeah and the fact
that it's fast and free

1011
00:36:58,912 --> 00:36:59,912
I think is gonna be pretty game changing.

1012
00:37:00,530 --> 00:37:01,530
There are photo realistic
image generators out there,

1013
00:37:05,126 --> 00:37:06,126
but a lot of them, they take a minute.

1014
00:37:06,432 --> 00:37:07,432
- They're hard to use.

1015
00:37:07,430 --> 00:37:08,430
You have to have the discord or whatever.

1016
00:37:09,190 --> 00:37:10,190
- Yeah and you have to
pay a subscription fee.

1017
00:37:10,832 --> 00:37:11,832
- [Interviewer] Yeah.

1018
00:37:11,830 --> 00:37:12,830
- So I think having it be free, fast,

1019
00:37:14,728 --> 00:37:15,728
able to exist in group chat threads.

1020
00:37:20,371 --> 00:37:21,371
I think people are just gonna

1021
00:37:21,204 --> 00:37:22,204
create a ton of images for fun.

1022
00:37:23,472 --> 00:37:24,472
And I don't know, I'm
really curious to see

1023
00:37:24,510 --> 00:37:25,510
how this gets used, but I
think it's gonna be super fun.

1024
00:37:27,180 --> 00:37:28,180
I mean, I already just
sit there with my kids

1025
00:37:32,305 --> 00:37:33,305
and we'll just keep on like,

1026
00:37:34,774 --> 00:37:35,774
I mean the word that you say to get it

1027
00:37:35,607 --> 00:37:36,607
to make an image is imagine.

1028
00:37:37,020 --> 00:37:38,020
And like my daughter's just like,

1029
00:37:39,190 --> 00:37:40,190
oh, I just wanna play imagine.

1030
00:37:40,192 --> 00:37:41,192
And I'm just like, imagine this.

1031
00:37:41,270 --> 00:37:42,270
And it's like we get
an image and it's like,

1032
00:37:42,491 --> 00:37:43,491
oh, well I actually wanna change it.

1033
00:37:44,007 --> 00:37:45,007
So imagine this and like edit the prompt.

1034
00:37:45,300 --> 00:37:46,300
But because it's just a
five second turnaround,

1035
00:37:47,632 --> 00:37:48,632
you could do that so
easily and you could do it

1036
00:37:50,640 --> 00:37:51,640
over the internet with group chat.

1037
00:37:53,131 --> 00:37:54,131
I'm not doing that sitting
there with my daughter,

1038
00:37:54,672 --> 00:37:55,672
but I think that that's
gonna be really fun.

1039
00:37:58,912 --> 00:37:59,912
So I think that there are all these things

1040
00:38:01,190 --> 00:38:02,190
where you can use these tools
to facilitate connections

1041
00:38:03,480 --> 00:38:04,480
and just create entertainment,

1042
00:38:06,192 --> 00:38:07,192
which is actually probably
more what the technology

1043
00:38:10,320 --> 00:38:11,320
is capable of today than even
some of the more utility use

1044
00:38:14,310 --> 00:38:15,310
cases because there is
the factuality issue.

1045
00:38:15,660 --> 00:38:16,660
- Yeah.

1046
00:38:17,172 --> 00:38:18,172
- But I mean with the
hallucinations and all that,

1047
00:38:18,005 --> 00:38:19,005
and we're trying to address
that by doing the partnerships

1048
00:38:22,710 --> 00:38:23,710
with search engines
right, that you mentioned.

1049
00:38:24,660 --> 00:38:25,660
So I mean you can type in a
question and ask real time

1050
00:38:29,490 --> 00:38:30,490
like who won this fight this
weekend and it'll be able

1051
00:38:31,590 --> 00:38:32,590
to go do a search and bring that in.

1052
00:38:33,960 --> 00:38:34,960
But there's still, you
know, I think hallucination

1053
00:38:37,931 --> 00:38:38,931
hasn't been solved
completely in any of these.

1054
00:38:41,651 --> 00:38:42,651
So I think to some degree
the thing that these language

1055
00:38:42,990 --> 00:38:43,990
models have really been
best at is, I mean,

1056
00:38:49,506 --> 00:38:50,506
it's kind of what the name Generative AI

1057
00:38:50,571 --> 00:38:51,571
suggests being generative, right?

1058
00:38:52,150 --> 00:38:53,150
Suggesting ideas, coming up
with things that could be

1059
00:38:54,660 --> 00:38:55,660
interesting or funny are
much better than like

1060
00:38:58,920 --> 00:38:59,920
I wouldn't necessarily yet
want it to be like my doctor

1061
00:39:01,440 --> 00:39:02,440
and ask it for a
diagnosis and have to rely

1062
00:39:03,240 --> 00:39:04,240
that it's not hallucinating.

1063
00:39:05,010 --> 00:39:06,010
So I think having it fit
into a consumer product

1064
00:39:08,250 --> 00:39:09,250
where the primary goals are-

1065
00:39:10,650 --> 00:39:11,650
- Interacting.

1066
00:39:11,483 --> 00:39:12,483
- Interesting content and
entertainment is actually

1067
00:39:13,511 --> 00:39:14,511
maybe a more natural fit
for what the technology

1068
00:39:16,260 --> 00:39:17,260
is capable of today than
some of the initial use cases

1069
00:39:19,530 --> 00:39:20,530
that people thought about.

1070
00:39:21,093 --> 00:39:22,093
It was like, oh, it's
gonna be this kind of like

1071
00:39:22,968 --> 00:39:23,968
all intelligent assistant
or it's gonna be my new

1072
00:39:24,330 --> 00:39:25,330
search engine or something.

1073
00:39:26,091 --> 00:39:27,091
- Right.

1074
00:39:27,094 --> 00:39:28,094
- I mean it's fine for
those a bunch of the time

1075
00:39:28,454 --> 00:39:29,454
and I think it will be,
it'll get there right,

1076
00:39:29,400 --> 00:39:30,400
over the next few years, but
I think the consumer thing

1077
00:39:33,072 --> 00:39:34,072
is actually quite a good fit today.

1078
00:39:34,170 --> 00:39:35,170
- It seems like a key
differentiator for Meta

1079
00:39:36,270 --> 00:39:37,270
in the whole model race is
you have probably second

1080
00:39:39,720 --> 00:39:40,720
to maybe Google the most
user data to train on.

1081
00:39:43,408 --> 00:39:44,408
And I know a lot of it's
private and you wouldn't train

1082
00:39:45,611 --> 00:39:46,611
on like ever train on like
private chats or WhatsApp.

1083
00:39:47,280 --> 00:39:48,280
- No, we don't.

1084
00:39:48,570 --> 00:39:49,570
- WhatsApp's encrypted too,

1085
00:39:49,620 --> 00:39:50,620
but like public stuff,
reels, public Facebook posts.

1086
00:39:53,460 --> 00:39:54,460
- Yeah.

1087
00:39:54,293 --> 00:39:55,293
- That seems pretty natural for this.

1088
00:39:56,430 --> 00:39:57,430
Is that in feeding Meta AI right now?

1089
00:40:00,251 --> 00:40:01,251
- Yeah, I mean, like you said,
we don't train on kind of

1090
00:40:05,264 --> 00:40:06,264
private chats that people
have with their friends

1091
00:40:06,844 --> 00:40:07,844
or things like that, but yeah-

1092
00:40:08,160 --> 00:40:09,160
- But you're sitting on this
just massive cord of data.

1093
00:40:12,219 --> 00:40:13,219
- Yeah, I-

1094
00:40:13,226 --> 00:40:14,226
- That could be interesting
in a model like this.

1095
00:40:15,821 --> 00:40:16,821
- I actually think a lot of
the stuff that we've done today

1096
00:40:17,760 --> 00:40:18,760
is actually still pretty basic.

1097
00:40:19,440 --> 00:40:20,440
So there's a lot of upside and
I think we need to experiment

1098
00:40:23,160 --> 00:40:24,160
with to see what ends up being useful.

1099
00:40:25,923 --> 00:40:26,923
But I mean, one of the things
that I think is interesting

1100
00:40:32,861 --> 00:40:33,861
is these AI problems, they're
kinda so tightly optimized

1101
00:40:37,470 --> 00:40:38,470
that having the AI basically
like live in the environment

1102
00:40:44,360 --> 00:40:45,360
that you're trying to
get it to, get better at,

1103
00:40:47,070 --> 00:40:48,070
is pretty important.

1104
00:40:48,240 --> 00:40:49,240
So like, so for example, you know,

1105
00:40:50,701 --> 00:40:51,701
you have things like chatGPT,

1106
00:40:51,720 --> 00:40:52,720
they're just in like a kind
of abstract chat interface,

1107
00:40:54,810 --> 00:40:55,810
but having it's getting
an AI to actually live

1108
00:40:59,400 --> 00:41:00,400
in a group chat for example,

1109
00:41:00,420 --> 00:41:01,420
is actually a completely different problem

1110
00:41:01,770 --> 00:41:02,770
because now you have
this question which is,

1111
00:41:03,923 --> 00:41:04,923
okay, when should the AI jump in, right?

1112
00:41:05,821 --> 00:41:06,821
And so it actually, like in
order to get an AI to be good

1113
00:41:09,939 --> 00:41:10,939
at being in a group chat,
you need to have experience

1114
00:41:11,670 --> 00:41:12,670
with AI's in group chats,

1115
00:41:12,870 --> 00:41:13,870
which even though like, I don't know,

1116
00:41:15,120 --> 00:41:16,120
Google or Open AI or other folks may have

1117
00:41:18,790 --> 00:41:19,790
a lot of experience with other things,

1118
00:41:20,630 --> 00:41:21,630
that kind of like
product dynamic of having

1119
00:41:23,190 --> 00:41:24,190
the actual experience that you're trying

1120
00:41:25,534 --> 00:41:26,534
to deliver the product in.

1121
00:41:27,513 --> 00:41:28,513
I think that that's super important.

1122
00:41:28,770 --> 00:41:29,770
Similarly, one of the things
that I'm pretty excited about,

1123
00:41:32,532 --> 00:41:33,532
I think multimodality is
pretty important interaction,

1124
00:41:34,320 --> 00:41:35,320
right?

1125
00:41:35,993 --> 00:41:36,993
I think a lot of these
things today are like,

1126
00:41:37,290 --> 00:41:38,290
okay you're an assistant,

1127
00:41:40,350 --> 00:41:41,350
I can chat with you in a box,

1128
00:41:42,870 --> 00:41:43,870
you don't change, right?

1129
00:41:44,894 --> 00:41:45,894
It's like you're the
same assistant every day.

1130
00:41:47,831 --> 00:41:48,831
And I think that that's
not really how people

1131
00:41:49,707 --> 00:41:50,707
tend to interact, right?

1132
00:41:51,310 --> 00:41:52,310
It's like in order to make
things fresh and entertaining,

1133
00:41:53,250 --> 00:41:54,250
even the apps that we
use, they change, right?

1134
00:41:55,740 --> 00:41:56,740
They get refreshed, they add new features

1135
00:41:58,313 --> 00:41:59,313
and I kind of think that
people will probably want

1136
00:42:05,033 --> 00:42:06,033
the AI's that they interact with.

1137
00:42:06,393 --> 00:42:07,393
I think it'll be more
exciting and interesting

1138
00:42:08,393 --> 00:42:09,393
if they do too.

1139
00:42:09,390 --> 00:42:10,390
So part of what I'm interested in

1140
00:42:11,040 --> 00:42:12,040
is this isn't just chat, right?

1141
00:42:14,073 --> 00:42:15,073
Chat I think will be where most
of the interaction happens,

1142
00:42:16,860 --> 00:42:17,860
but these AI's are gonna
have profiles and Instagram

1143
00:42:20,610 --> 00:42:21,610
and Facebook and they'll
be able to post content

1144
00:42:23,250 --> 00:42:24,250
and they'll be able to
interact with people

1145
00:42:24,690 --> 00:42:25,690
and interact with each other, right?

1146
00:42:27,120 --> 00:42:28,120
And I think that there's
this whole like interesting

1147
00:42:30,120 --> 00:42:31,120
set of flywheels around how
that interaction can happen

1148
00:42:33,450 --> 00:42:34,450
and how they can sort of evolve over time

1149
00:42:35,400 --> 00:42:36,400
and I think that that's
gonna be very compelling

1150
00:42:39,450 --> 00:42:40,450
and interesting and I mean
obviously we're kind of starting

1151
00:42:43,273 --> 00:42:44,273
slowly on that, but I
think that having them sort

1152
00:42:46,414 --> 00:42:47,414
of exist in that environment

1153
00:42:49,434 --> 00:42:50,434
and then, you know, including,

1154
00:42:50,267 --> 00:42:51,267
so we wanted to build it
so that it kind of worked

1155
00:42:52,393 --> 00:42:53,393
across the whole Meta
universe of products,

1156
00:42:54,390 --> 00:42:55,390
including having them be
able to in the near future

1157
00:42:57,540 --> 00:42:58,540
be embodied as avatars
in the Metaverse, right?

1158
00:43:00,452 --> 00:43:01,452
So you go into VR and you
have an avatar version

1159
00:43:03,180 --> 00:43:04,180
of the AI and you can talk to them there.

1160
00:43:05,972 --> 00:43:06,972
I think that that's gonna
be really compelling, right?

1161
00:43:09,249 --> 00:43:10,249
It's at a minimum
creating much better NPCs

1162
00:43:12,270 --> 00:43:13,270
and experiences when there
isn't another actual person

1163
00:43:16,680 --> 00:43:17,680
who you wanna play a game with.

1164
00:43:18,633 --> 00:43:19,633
You can just have AI's that
are much more realistic

1165
00:43:20,640 --> 00:43:21,640
and kind of compelling to interact with.

1166
00:43:23,490 --> 00:43:24,490
But I dunno, I think having this crossover

1167
00:43:25,800 --> 00:43:26,800
where like you have an
assistant or you have some,

1168
00:43:29,251 --> 00:43:30,251
I don't know, someone who tells you jokes

1169
00:43:33,273 --> 00:43:34,273
and kind of cracks you
up and entertains you

1170
00:43:34,230 --> 00:43:35,230
and then like they can show
up in some of your metaverse

1171
00:43:36,810 --> 00:43:37,810
worlds and be able to
be there as an avatar,

1172
00:43:39,360 --> 00:43:40,360
but you can still interact
with them in the same way,

1173
00:43:41,614 --> 00:43:42,614
I think it's pretty cool.

1174
00:43:44,190 --> 00:43:45,190
- So you think the advent
of these AI personas

1175
00:43:45,450 --> 00:43:46,450
that are way more intelligent
will accelerate interest

1176
00:43:49,650 --> 00:43:50,650
in the Metaverse and VR?

1177
00:43:53,369 --> 00:43:54,369
- Well, I think that all this stuff

1178
00:43:54,410 --> 00:43:55,410
makes it more compelling.

1179
00:43:58,254 --> 00:43:59,254
So I'm not, I mean, so yes,
I think that this will kind

1180
00:44:03,913 --> 00:44:04,913
of make all these things more useful.

1181
00:44:05,372 --> 00:44:06,372
- [Interviewer] Yeah.

1182
00:44:06,372 --> 00:44:07,372
- I think it's probably
an even bigger deal

1183
00:44:07,650 --> 00:44:08,650
for smart glasses than for VR.

1184
00:44:12,450 --> 00:44:13,450
- You need something,
you need a kind of visual

1185
00:44:13,800 --> 00:44:14,800
or a voice control for someone.

1186
00:44:16,254 --> 00:44:17,254
- Well I kind of thought, you know,

1187
00:44:18,036 --> 00:44:19,036
when I was thinking about
what would be the key features

1188
00:44:19,800 --> 00:44:20,800
for smart glasses, I kind of
thought that we were gonna

1189
00:44:23,070 --> 00:44:24,070
get holograms in the
world and that was one,

1190
00:44:26,430 --> 00:44:27,430
so that's kind of like augmented reality,

1191
00:44:28,620 --> 00:44:29,620
but then there was always
some sort of vague notion

1192
00:44:33,540 --> 00:44:34,540
that you'd have like an assistant
that could do something.

1193
00:44:37,614 --> 00:44:38,614
But I thought that
things like Siri or Alexa

1194
00:44:43,796 --> 00:44:44,796
were very limited, right?

1195
00:44:44,629 --> 00:44:45,629
So I was just kinda like, okay,

1196
00:44:46,676 --> 00:44:47,676
well like over the time
period of building AR glasses,

1197
00:44:48,120 --> 00:44:49,120
like hopefully the AI will advance

1198
00:44:51,076 --> 00:44:52,076
and now it definitely has.

1199
00:44:52,132 --> 00:44:53,132
- Yeah.

1200
00:44:53,134 --> 00:44:54,134
- So now I think we're at this point

1201
00:44:53,967 --> 00:44:54,967
where it may actually be the
case that for smart glasses,

1202
00:44:56,940 --> 00:44:57,940
the AI is compelling before the holograms

1203
00:44:59,220 --> 00:45:00,220
and the displays are, which
is sort of where we got to

1204
00:45:03,193 --> 00:45:04,193
with the new version of the
Ray-Bans that we shipped

1205
00:45:07,888 --> 00:45:08,888
that we're shipping this year, right?

1206
00:45:09,531 --> 00:45:10,531
It's when we started
working on the product,

1207
00:45:11,280 --> 00:45:12,280
all this generative AI
stuff hadn't happened yet.

1208
00:45:14,511 --> 00:45:15,511
So we actually started
working on the product

1209
00:45:16,156 --> 00:45:17,156
just as an improvement over
the first generation, right?

1210
00:45:19,231 --> 00:45:20,231
So that the photos are better,
the audio's a lot better,

1211
00:45:22,260 --> 00:45:23,260
like the form factor's
better, it's lighter.

1212
00:45:28,276 --> 00:45:29,276
So it's just sort of like
a much more refined version

1213
00:45:30,281 --> 00:45:31,281
of the initial product.

1214
00:45:31,278 --> 00:45:32,278
And there's some new features
like you can live stream now,

1215
00:45:33,161 --> 00:45:34,161
which is pretty cool right,

1216
00:45:34,158 --> 00:45:35,158
you can live stream
what you're looking at.

1217
00:45:36,017 --> 00:45:37,017
But it was only over the course
of developing the product

1218
00:45:40,180 --> 00:45:41,180
that we realized that
hey, we could actually put

1219
00:45:41,298 --> 00:45:42,298
this whole generative AI assistant into it

1220
00:45:45,477 --> 00:45:46,477
and you could have these
glasses that are kind of stylish

1221
00:45:48,180 --> 00:45:49,180
Ray-Ban glasses and you
could be talking to AI

1222
00:45:52,041 --> 00:45:53,041
all throughout the day about
different questions you have.

1223
00:45:55,582 --> 00:45:56,582
And you know this isn't in
the first software release,

1224
00:45:58,003 --> 00:45:59,003
but sometime early next year

1225
00:46:00,004 --> 00:46:01,004
we're also gonna have this multimodality.

1226
00:46:01,100 --> 00:46:02,100
So you're gonna be able to ask the AI,

1227
00:46:03,270 --> 00:46:04,270
Hey, what is it that I'm looking at?

1228
00:46:04,500 --> 00:46:05,500
Like what type of plant is that?

1229
00:46:07,230 --> 00:46:08,230
Like where am I in order to use a camera?

1230
00:46:10,482 --> 00:46:11,482
Like how expensive is this thing?

1231
00:46:11,481 --> 00:46:12,481
- Yeah, I mean it has a
camera built into the glasses.

1232
00:46:12,720 --> 00:46:13,720
So you can just like look at
something and you're like,

1233
00:46:15,625 --> 00:46:16,625
all right, and you're filming
with some Canon camera.

1234
00:46:18,873 --> 00:46:19,873
It's like where do I get one of those?

1235
00:46:21,270 --> 00:46:22,270
I think that that's,

1236
00:46:22,761 --> 00:46:23,761
I think it'd be very interesting.

1237
00:46:25,342 --> 00:46:26,342
- Yeah.

1238
00:46:26,356 --> 00:46:27,356
- And again, this is all
like really novel stuff,

1239
00:46:27,705 --> 00:46:28,705
so I'm not pretending to know
exactly what the key use case

1240
00:46:31,902 --> 00:46:32,902
is or how people are gonna use that.

1241
00:46:33,225 --> 00:46:34,225
But I think the idea,

1242
00:46:36,164 --> 00:46:37,164
smart glasses are very powerful for AI

1243
00:46:39,450 --> 00:46:40,450
because unlike having it on your phone,

1244
00:46:43,042 --> 00:46:44,042
glasses as a form factor
can see what you see

1245
00:46:47,940 --> 00:46:48,940
and hear what you hear
from your perspective.

1246
00:46:51,417 --> 00:46:52,417
So if you wanna build an AI assistant

1247
00:46:53,401 --> 00:46:54,401
that really has access
to all of the inputs

1248
00:46:55,380 --> 00:46:56,380
that you have as a person,

1249
00:46:57,270 --> 00:46:58,270
glasses are probably the way
that you wanna build that.

1250
00:47:00,542 --> 00:47:01,542
- [Interviewer] Yeah.

1251
00:47:01,375 --> 00:47:02,375
- And so, I don't know, I
think that that's just really

1252
00:47:04,219 --> 00:47:05,219
interesting and compelling,

1253
00:47:05,070 --> 00:47:06,070
and it's sort of this whole
new angle on smart glasses

1254
00:47:08,644 --> 00:47:09,644
that I thought might materialize

1255
00:47:09,510 --> 00:47:10,510
over a five to 10 year period.

1256
00:47:11,764 --> 00:47:12,764
But in this odd twist
of the tech industry,

1257
00:47:14,334 --> 00:47:15,334
I think actually is gonna show up

1258
00:47:15,780 --> 00:47:16,780
maybe before even super
high quality holograms do.

1259
00:47:19,050 --> 00:47:20,050
- Is overall interest in the
Ray Bands and the Quest line

1260
00:47:24,090 --> 00:47:25,090
kind of tracking with where
you thought it would be

1261
00:47:28,382 --> 00:47:29,382
at this point?

1262
00:47:31,182 --> 00:47:32,182
- Well, I think you,

1263
00:47:32,201 --> 00:47:33,201
let's take each of those separately.

1264
00:47:33,034 --> 00:47:34,034
- Yeah, they're separate products, but-

1265
00:47:34,959 --> 00:47:35,959
- So for Quest, we had
Quest One was the first

1266
00:47:39,321 --> 00:47:40,321
kind of standalone
product, and it did well,

1267
00:47:44,242 --> 00:47:45,242
but all the content had
to be developed for it.

1268
00:47:46,100 --> 00:47:47,100
So it was really when
we developed Quest Two,

1269
00:47:48,180 --> 00:47:49,180
which was the next generation of it

1270
00:47:50,761 --> 00:47:51,761
that already had all the content built,

1271
00:47:51,594 --> 00:47:52,594
and it was sort of the
kind of refinement on it,

1272
00:47:55,440 --> 00:47:56,440
that one blew up.

1273
00:47:56,280 --> 00:47:57,280
So Quest Two was like a huge hit,

1274
00:47:58,542 --> 00:47:59,542
tens of millions, right?

1275
00:47:59,902 --> 00:48:00,902
And just that did very
well and was sort of like

1276
00:48:03,742 --> 00:48:04,742
the kind of defining VR device so far.

1277
00:48:08,121 --> 00:48:09,121
Then we shipped Quest Pro,

1278
00:48:09,284 --> 00:48:10,284
which was making the
leap to mixed reality,

1279
00:48:11,982 --> 00:48:12,982
but it was $1,500.

1280
00:48:13,124 --> 00:48:14,124
And what we've seen so far is that

1281
00:48:16,462 --> 00:48:17,462
at least consumers are
very cost conscious.

1282
00:48:19,182 --> 00:48:20,182
So we didn't expect to sell,

1283
00:48:20,790 --> 00:48:21,790
well sorry, we expected to
sell way fewer Quest Pros

1284
00:48:24,142 --> 00:48:25,142
than Quest Twos and that beared out.

1285
00:48:26,318 --> 00:48:27,318
- Yeah.

1286
00:48:27,151 --> 00:48:28,151
- So I think it was like,

1287
00:48:27,984 --> 00:48:28,984
it's always hard to predict
exactly what it'll be when

1288
00:48:29,880 --> 00:48:30,880
you're shipping a product at
$1,500 for the first time.

1289
00:48:32,940 --> 00:48:33,940
But like, I'd say it was
kinda fine with expectations.

1290
00:48:37,140 --> 00:48:38,140
It wasn't like a grand slam,
but it was, it did fine.

1291
00:48:41,010 --> 00:48:42,010
And now Quest Three is
sort of the refinement

1292
00:48:44,657 --> 00:48:45,657
on mixed reality, kind
of like Quest One was,

1293
00:48:48,158 --> 00:48:49,158
but with Quest Three
we're sort of at the point

1294
00:48:51,022 --> 00:48:52,022
where we've gotten mixed reality,

1295
00:48:52,100 --> 00:48:53,100
which is even higher quality
than what was in Quest Pro,

1296
00:48:54,300 --> 00:48:55,300
but it's a third of the
price right, so it's $500.

1297
00:48:57,194 --> 00:48:58,194
So I'm really excited to
see how that one will go.

1298
00:48:59,640 --> 00:49:00,640
- It seems like you
all, based on my demos,

1299
00:49:02,160 --> 00:49:03,160
still kind of primarily think
of it as a gaming device.

1300
00:49:05,742 --> 00:49:06,742
Is that fair that the main
use cases for Quest Three

1301
00:49:08,340 --> 00:49:09,340
are gonna be, and these
kind of gaming meets social,

1302
00:49:11,674 --> 00:49:12,674
so you've got Roblox now.

1303
00:49:12,507 --> 00:49:13,507
- Yeah, yeah, so I think social
is actually the first thing,

1304
00:49:15,720 --> 00:49:16,720
which is interesting.

1305
00:49:17,296 --> 00:49:18,296
I mean cuz Quest used
to be primarily gaming,

1306
00:49:20,250 --> 00:49:21,250
and now if you look at what experiences

1307
00:49:22,290 --> 00:49:23,290
are people spending the most time in,

1308
00:49:23,940 --> 00:49:24,940
it's actually just different

1309
00:49:25,838 --> 00:49:26,838
social metaverse type experiences.

1310
00:49:27,561 --> 00:49:28,561
So things like Rec Room,
VR Chat, Horizon, Roblox,

1311
00:49:32,340 --> 00:49:33,340
when it shows up, but that's
not, I mean, it's there.

1312
00:49:34,800 --> 00:49:35,800
It's starting to scale, but I think Roblox

1313
00:49:36,900 --> 00:49:37,900
can be a lot bigger than it is.

1314
00:49:38,160 --> 00:49:39,160
But even with Roblox just
kind of starting to grow

1315
00:49:41,758 --> 00:49:42,758
on the platform, social
is already more time spent

1316
00:49:43,290 --> 00:49:44,290
than gaming use cases.

1317
00:49:45,081 --> 00:49:46,081
- Hmm.

1318
00:49:46,078 --> 00:49:47,078
- So now it's different if
you look at the economics,

1319
00:49:48,630 --> 00:49:49,630
because people pay more for games.

1320
00:49:50,875 --> 00:49:51,875
- Sure.

1321
00:49:51,872 --> 00:49:52,872
- Whereas social kind of has
that whole adoption curve thing

1322
00:49:52,740 --> 00:49:53,740
that I talked about before
where first you have to kind of

1323
00:49:56,092 --> 00:49:57,092
build out the big community and
then you can enable commerce

1324
00:49:57,930 --> 00:49:58,930
and kinda monetize it over time.

1325
00:49:59,880 --> 00:50:00,880
But this was, I mean, this is
sort of my whole theory for VR

1326
00:50:02,970 --> 00:50:03,970
was people looked at it
initially as a gaming device,

1327
00:50:05,835 --> 00:50:06,835
and I thought, Hey, like I think this

1328
00:50:10,875 --> 00:50:11,875
is a new computing platform overall.

1329
00:50:11,708 --> 00:50:12,708
Computing platforms tend to be
good for three major things.

1330
00:50:15,090 --> 00:50:16,090
Gaming, social and
communication and productivity.

1331
00:50:18,420 --> 00:50:19,420
And I'm pretty sure we
can nail the social one.

1332
00:50:20,760 --> 00:50:21,760
If we can find the right
partners on productivity,

1333
00:50:22,950 --> 00:50:23,950
and if we can support
the gaming ecosystem,

1334
00:50:24,840 --> 00:50:25,840
then I think that we can
help this become a big thing.

1335
00:50:27,456 --> 00:50:28,456
So I'd say broadly
that's sort of on track.

1336
00:50:29,400 --> 00:50:30,400
I mean, it was like,
I thought it was gonna

1337
00:50:32,511 --> 00:50:33,511
be a long-term project, but
I think the fact that social

1338
00:50:35,310 --> 00:50:36,310
has now overtaken gaming is
the most thing that people

1339
00:50:40,613 --> 00:50:41,613
are spending the most time on,

1340
00:50:41,788 --> 00:50:42,788
is an interesting software
evolution in how they're used.

1341
00:50:43,140 --> 00:50:44,140
- Mm-hmm.

1342
00:50:44,370 --> 00:50:45,370
- But yeah, like you're
saying, I mean, entertainment,

1343
00:50:47,070 --> 00:50:48,070
social, gaming, still the primary things,

1344
00:50:49,260 --> 00:50:50,260
productivity I think
still needs some time.

1345
00:50:52,690 --> 00:50:53,690
- Yeah.

1346
00:50:53,829 --> 00:50:54,829
- To develop.

1347
00:50:55,589 --> 00:50:56,589
- Yeah, I tried the Quest three.

1348
00:50:56,613 --> 00:50:57,613
It's definitely a meaningful
step change in terms of

1349
00:50:59,189 --> 00:51:00,189
graphics and performance
and all the things

1350
00:51:00,030 --> 00:51:01,030
you guys have put into it.

1351
00:51:01,920 --> 00:51:02,920
It feels still like
we're a little ways away

1352
00:51:04,860 --> 00:51:05,860
from this medium becoming
truly mainstream,

1353
00:51:08,130 --> 00:51:09,130
becoming something that new.

1354
00:51:10,411 --> 00:51:11,411
- When you say mainstream do you mean-

1355
00:51:11,792 --> 00:51:12,792
- Well, I know you're already
at kind of console level

1356
00:51:14,448 --> 00:51:15,448
sales, so you could say that's mainstream,

1357
00:51:15,750 --> 00:51:16,750
but I guess in terms of
like what you could think of

1358
00:51:18,731 --> 00:51:19,731
as a general purpose computing platform.

1359
00:51:19,860 --> 00:51:20,860
So even like PC or something
like that, like, seems-

1360
00:51:23,940 --> 00:51:24,940
- Well in what sense?

1361
00:51:25,552 --> 00:51:26,552
I think there's a few parts of this.

1362
00:51:27,733 --> 00:51:28,733
I think for productivity,

1363
00:51:29,687 --> 00:51:30,687
you probably want somewhat
higher resolution screens.

1364
00:51:32,373 --> 00:51:33,373
- Right.

1365
00:51:33,206 --> 00:51:34,206
- Right?

1366
00:51:34,373 --> 00:51:35,373
So I think that that's,
and that I think will come,

1367
00:51:36,251 --> 00:51:37,251
and I think we're waiting for
the cost curve to basically,

1368
00:51:38,929 --> 00:51:39,929
we could have super high
resolution screens today,

1369
00:51:42,048 --> 00:51:43,048
it's just that the
device would be thousands

1370
00:51:43,989 --> 00:51:44,989
and thousands of dollars right,

1371
00:51:45,131 --> 00:51:46,131
which is basically the trade
off that that Apple made

1372
00:51:46,971 --> 00:51:47,971
with with their Vision Pro.

1373
00:51:48,213 --> 00:51:49,213
- Have you tried it yet?

1374
00:51:49,211 --> 00:51:50,211
- No, I haven't, no.

1375
00:51:50,044 --> 00:51:51,044
- Yeah.

1376
00:51:51,211 --> 00:51:52,211
- But, but I-

1377
00:51:52,227 --> 00:51:53,227
- You're right, they guided
towards that one spec.

1378
00:51:54,450 --> 00:51:55,450
You can tell that they
guided towards resolution.

1379
00:51:57,000 --> 00:51:58,000
- Yeah and you just have to
imagine that over the next,

1380
00:51:59,430 --> 00:52:00,430
you know, five plus years,
like there will be displays

1381
00:52:03,270 --> 00:52:04,270
that are that good and
they'll come down in cost,

1382
00:52:05,460 --> 00:52:06,460
and we're just sort of riding that curve.

1383
00:52:09,151 --> 00:52:10,151
But I know, so for today,

1384
00:52:09,984 --> 00:52:10,984
when you're building
one of these products,

1385
00:52:11,070 --> 00:52:12,070
you basically have the
choice of if you have it

1386
00:52:14,752 --> 00:52:15,752
at that expensive, then
you'll sell hundreds

1387
00:52:16,380 --> 00:52:17,380
of thousands of units or something.

1388
00:52:18,987 --> 00:52:19,987
But we're trying to build
something where we build up

1389
00:52:22,432 --> 00:52:23,432
the community of people using it.

1390
00:52:24,868 --> 00:52:25,868
So we're trying to thread the needle

1391
00:52:27,312 --> 00:52:28,312
and have the best possible
display that we can

1392
00:52:28,710 --> 00:52:29,710
while having it cost $500, not $3,500.

1393
00:52:32,100 --> 00:52:33,100
- Yeah, I reported on this some comments

1394
00:52:33,690 --> 00:52:34,690
you made to employees after
Apple debuted Division Pro,

1395
00:52:37,093 --> 00:52:38,093
and you didn't seem super phased by it.

1396
00:52:39,186 --> 00:52:40,186
Like it seemed like it didn't bother you

1397
00:52:40,320 --> 00:52:41,320
as much as it maybe could have.

1398
00:52:41,940 --> 00:52:42,940
I have to imagine if they
released a $700 headset

1399
00:52:45,360 --> 00:52:46,360
we would be a diff we'd be
having a different conversation.

1400
00:52:49,609 --> 00:52:50,609
- But yeah, I mean-

1401
00:52:51,189 --> 00:52:52,189
- There should be low
volume and they're probably

1402
00:52:53,243 --> 00:52:54,243
three to four years out of a
general, like a lower tier type

1403
00:52:56,250 --> 00:52:57,250
release that's at any meaningful scale.

1404
00:52:58,230 --> 00:52:59,230
So I guess, I mean, is it
because the market's yours

1405
00:53:00,300 --> 00:53:01,300
foreseeably then for awhile?

1406
00:53:02,886 --> 00:53:03,886
- Well, I mean Apple is
obviously very good at this,

1407
00:53:06,138 --> 00:53:07,138
so I don't wanna be dismissive,

1408
00:53:07,142 --> 00:53:08,142
but because we're relatively
newer to building this,

1409
00:53:10,918 --> 00:53:11,918
the thing that I wasn't sure
about is when Apple released

1410
00:53:13,020 --> 00:53:14,020
a device, were they just gonna
have made some completely

1411
00:53:17,610 --> 00:53:18,610
new insight or breakthrough
that just made our effort -

1412
00:53:22,481 --> 00:53:23,481
- Blew your R&D up.

1413
00:53:23,478 --> 00:53:24,478
- Yeah, that it was just like, oh, like,

1414
00:53:24,311 --> 00:53:25,311
well now we need to go
start over or something.

1415
00:53:26,610 --> 00:53:27,610
And to me that was the
thing that I thought

1416
00:53:30,738 --> 00:53:31,738
we were doing pretty good work,

1417
00:53:31,571 --> 00:53:32,571
so I thought that was unlikely,

1418
00:53:33,062 --> 00:53:34,062
but you don't know for
sure until you see it.

1419
00:53:35,499 --> 00:53:36,499
They show up with their thing

1420
00:53:36,662 --> 00:53:37,662
and there was just nothing like that.

1421
00:53:38,010 --> 00:53:39,010
Right, so I think that
there are some things

1422
00:53:40,110 --> 00:53:41,110
that they did that are clever.

1423
00:53:41,820 --> 00:53:42,820
I think, well when we
actually get to use it more,

1424
00:53:45,059 --> 00:53:46,059
I'm sure that there are
gonna be other things

1425
00:53:46,982 --> 00:53:47,982
that we'll learn that are interesting,

1426
00:53:47,815 --> 00:53:48,815
but mostly they just chose a
different part of the market

1427
00:53:50,760 --> 00:53:51,760
to go in it's sort of,

1428
00:53:52,320 --> 00:53:53,320
I think it makes sense for them, right?

1429
00:53:54,539 --> 00:53:55,539
I mean I think that they sell,

1430
00:53:55,825 --> 00:53:56,825
it must be what 15 to 20
million MacBooks a year.

1431
00:53:57,870 --> 00:53:58,870
And from their perspective
if they can replace those

1432
00:54:02,699 --> 00:54:03,699
MacBooks over time with
things like Vision Pro,

1433
00:54:04,560 --> 00:54:05,560
then that's like a pretty
good business for them.

1434
00:54:07,158 --> 00:54:08,158
- [Interviewer] Yeah.

1435
00:54:08,182 --> 00:54:09,182
- Right?

1436
00:54:09,179 --> 00:54:10,179
And it'll be many billions
of dollars of revenue.

1437
00:54:10,080 --> 00:54:11,080
And I think they're pretty happy selling

1438
00:54:11,760 --> 00:54:12,760
20 million or 15 million MacBooks a year.

1439
00:54:14,730 --> 00:54:15,730
It's good.

1440
00:54:15,870 --> 00:54:16,870
But we play a different game, right?

1441
00:54:18,090 --> 00:54:19,090
I mean we're not trying to
sell devices at a big premium

1442
00:54:20,820 --> 00:54:21,820
and make a ton of money on the devices.

1443
00:54:23,121 --> 00:54:24,121
Going back to the curve that
we were talking about before,

1444
00:54:26,730 --> 00:54:27,730
we wanna build something that's great,

1445
00:54:28,560 --> 00:54:29,560
get it to be so that people use it

1446
00:54:31,020 --> 00:54:32,020
and want to use it like
every week and every day,

1447
00:54:33,660 --> 00:54:34,660
and then over time scale
it to hundreds of millions

1448
00:54:36,780 --> 00:54:37,780
or billions of people.

1449
00:54:38,380 --> 00:54:39,380
And I think if you want to do that,

1450
00:54:40,530 --> 00:54:41,530
then you have to innovate
not just on the quality

1451
00:54:43,158 --> 00:54:44,158
of the device, but also
in making it affordable

1452
00:54:44,700 --> 00:54:45,700
and accessible to people.

1453
00:54:46,518 --> 00:54:47,518
So I do just think we're
playing somewhat different games

1454
00:54:48,600 --> 00:54:49,600
and that I think makes it, that over time,

1455
00:54:51,480 --> 00:54:52,480
they'll I'm sure build a high
quality device in the zone

1456
00:54:55,782 --> 00:54:56,782
that they're focusing on and
it may just be that these

1457
00:54:57,660 --> 00:54:58,660
are in fairly different
spaces for a long time.

1458
00:54:59,850 --> 00:55:00,850
- Yeah.

1459
00:55:01,382 --> 00:55:02,382
- But I'm not sure.

1460
00:55:02,379 --> 00:55:03,379
I think we'll see as it goes.

1461
00:55:04,342 --> 00:55:05,342
- You could lean too much
I guess into the Android

1462
00:55:05,250 --> 00:55:06,250
versus iOS analogy here,

1463
00:55:07,318 --> 00:55:08,318
but yeah I guess where
do you see that going?

1464
00:55:10,860 --> 00:55:11,860
Does Meta really lean into
and the Android approach

1465
00:55:14,454 --> 00:55:15,454
and you start licensing
your software and technology

1466
00:55:17,041 --> 00:55:18,041
to other early agents?

1467
00:55:18,038 --> 00:55:19,038
- Yeah, I'd like to have this be

1468
00:55:18,871 --> 00:55:19,871
a more open ecosystem over time.

1469
00:55:21,030 --> 00:55:22,030
I mean my theory on how these
computing platforms evolve

1470
00:55:26,160 --> 00:55:27,160
is there will be a closed integrated stack

1471
00:55:28,950 --> 00:55:29,950
and a more open stack.

1472
00:55:30,180 --> 00:55:31,180
And there have been in every
generation of computing so far.

1473
00:55:34,260 --> 00:55:35,260
The thing that's actually not clear

1474
00:55:36,300 --> 00:55:37,300
is which one will end up being
the more successful, right?

1475
00:55:38,820 --> 00:55:39,820
I think we're looking,

1476
00:55:39,870 --> 00:55:40,870
we're kind of coming off
of the mobile one now,

1477
00:55:42,840 --> 00:55:43,840
where Apple has truly
been the dominant company

1478
00:55:46,020 --> 00:55:47,020
where even though there are
technically more Android phones,

1479
00:55:48,742 --> 00:55:49,742
there's way more economic
activity in the center of gravity

1480
00:55:50,490 --> 00:55:51,490
for all this stuff is clearly on iPhones.

1481
00:55:52,481 --> 00:55:53,481
In a lot of the most important
countries for defining this,

1482
00:55:57,150 --> 00:55:58,150
I think iPhone has a
majority and growing share

1483
00:56:00,390 --> 00:56:01,390
and I think it's clearly
just the dominant company

1484
00:56:02,220 --> 00:56:03,220
in the space,

1485
00:56:03,819 --> 00:56:04,819
but that wasn't true in computers and PCs.

1486
00:56:07,050 --> 00:56:08,050
- Right.

1487
00:56:08,040 --> 00:56:09,040
Microsoft.

1488
00:56:09,739 --> 00:56:10,739
- And so our approach here is to focus

1489
00:56:12,150 --> 00:56:13,150
on making it as affordable as possible.

1490
00:56:15,060 --> 00:56:16,060
We want to be the open ecosystem

1491
00:56:17,070 --> 00:56:18,070
and we want the open ecosystem to win.

1492
00:56:19,579 --> 00:56:20,579
- [Interviewer] Yeah.

1493
00:56:20,598 --> 00:56:21,598
- Right?

1494
00:56:21,601 --> 00:56:22,601
So I think it is possible that
this will be more like PCs

1495
00:56:24,330 --> 00:56:25,330
than like mobile where
it's like where maybe Apple

1496
00:56:30,118 --> 00:56:31,118
goes for this for kind
of a high end segment,

1497
00:56:34,241 --> 00:56:35,241
and maybe we end up being the
kind of the primary ecosystem

1498
00:56:39,090 --> 00:56:40,090
and the one that ends up
serving billions of people.

1499
00:56:45,418 --> 00:56:46,418
That's the outcome that
we're sort of playing for.

1500
00:56:48,822 --> 00:56:49,822
- On the progress that you're
making with AR glasses,

1501
00:56:51,441 --> 00:56:52,441
it's my understanding that
you're gonna have your first

1502
00:56:53,302 --> 00:56:54,302
kind of internal at
least dev kit next year.

1503
00:56:55,170 --> 00:56:56,170
I don't know if you're gonna
show it off publicly or not

1504
00:56:57,638 --> 00:56:58,638
if that's been decided, but is
that progressing at the rate

1505
00:57:00,754 --> 00:57:01,754
that you have hoped as well?

1506
00:57:02,438 --> 00:57:03,438
It seems like Apple's dealt with this,

1507
00:57:03,420 --> 00:57:04,420
that everyone's been dealing
with kind of the technical

1508
00:57:06,330 --> 00:57:07,330
problems with this.

1509
00:57:07,862 --> 00:57:08,862
- I don't think we have,

1510
00:57:08,859 --> 00:57:09,859
I don't think I have anything
to announce on that today.

1511
00:57:14,557 --> 00:57:15,557
- You said AR glasses are a kind of

1512
00:57:15,690 --> 00:57:16,690
end of this decade thing.

1513
00:57:17,918 --> 00:57:18,918
And I guess what I'm trying to get at is-

1514
00:57:18,780 --> 00:57:19,780
- To be at more of a
mainstream consumer product,

1515
00:57:23,400 --> 00:57:24,400
not like a V1.

1516
00:57:24,720 --> 00:57:25,720
But I don't have anything new
to announce today on this.

1517
00:57:29,380 --> 00:57:30,380
And we have a bunch of versions of this

1518
00:57:30,270 --> 00:57:31,270
that we're building internally.

1519
00:57:34,481 --> 00:57:35,481
We're kind of coming at it
from two angles at once.

1520
00:57:36,030 --> 00:57:37,030
We're starting with
Ray-Ban, which is like,

1521
00:57:38,758 --> 00:57:39,758
all right if you take
stylish glasses today,

1522
00:57:41,233 --> 00:57:42,233
what's the most technology
that you can cram into that

1523
00:57:43,500 --> 00:57:44,500
and make it a good product?

1524
00:57:46,057 --> 00:57:47,057
And then we're coming at
it from the other side,

1525
00:57:47,521 --> 00:57:48,521
which is like, all
right, we want to create

1526
00:57:49,156 --> 00:57:50,156
what is our ideal product
with like full holograms,

1527
00:57:53,318 --> 00:57:54,318
you walk into a room, like
there's like as many holograms

1528
00:57:56,310 --> 00:57:57,310
there as there are physical objects,

1529
00:58:00,674 --> 00:58:01,674
like you can interact with
like people as holograms,

1530
00:58:03,450 --> 00:58:04,450
AI's as holograms, like all this stuff.

1531
00:58:05,700 --> 00:58:06,700
And then how do we get that
to basically fit into glasses

1532
00:58:10,200 --> 00:58:11,200
like form factor at as affordable

1533
00:58:14,730 --> 00:58:15,730
of a price as we can get to.

1534
00:58:17,382 --> 00:58:18,382
And I'd say the Ray-Ban one,

1535
00:58:20,280 --> 00:58:21,280
I'm really curious to see
how the second generation

1536
00:58:23,040 --> 00:58:24,040
of the Ray-Bans does.

1537
00:58:24,030 --> 00:58:25,030
And the first one I think the
reception was pretty good.

1538
00:58:28,320 --> 00:58:29,320
I mean there was a bunch of
reports about the retention,

1539
00:58:31,140 --> 00:58:32,140
right, being somewhat lower, right.

1540
00:58:33,156 --> 00:58:34,156
And yeah I think that
there's a bunch of stuff

1541
00:58:35,016 --> 00:58:36,016
that we just need to
polish where it's like

1542
00:58:36,998 --> 00:58:37,998
the cameras are just so much better.

1543
00:58:37,831 --> 00:58:38,831
The audio is so much better.

1544
00:58:39,558 --> 00:58:40,558
A lot of people, and we didn't
realize that a lot of people

1545
00:58:42,459 --> 00:58:43,459
were gonna wanna use it for
like listening to podcasts

1546
00:58:43,950 --> 00:58:44,950
when they go on a run, right?

1547
00:58:45,601 --> 00:58:46,601
That wasn't what we designed it for,

1548
00:58:46,500 --> 00:58:47,500
but it was a great use case.

1549
00:58:48,000 --> 00:58:49,000
So it's like, okay, yeah great.

1550
00:58:49,140 --> 00:58:50,140
Like let's make sure that's good in V2.

1551
00:58:51,953 --> 00:58:52,953
So it's the cycle for iterating on this,

1552
00:58:53,670 --> 00:58:54,670
if you're doing like a
threads release or Instagram,

1553
00:58:57,840 --> 00:58:58,840
the cycle's like a month.

1554
00:58:58,890 --> 00:58:59,890
- Yeah, it's very different.

1555
00:59:00,699 --> 00:59:01,699
- For hardware it's like
18 months, two years.

1556
00:59:02,239 --> 00:59:03,239
So it's but I think this is the next step

1557
00:59:04,620 --> 00:59:05,620
and I think we're gonna just
gonna climb up that curve.

1558
00:59:06,900 --> 00:59:07,900
But the initial interest I think is there,

1559
00:59:12,194 --> 00:59:13,194
I think this is an interesting
base to build from,

1560
00:59:14,299 --> 00:59:15,299
so I feel good about that.

1561
00:59:16,401 --> 00:59:17,401
Going the other direction,

1562
00:59:18,358 --> 00:59:19,358
I mean the technology is hard, right?

1563
00:59:19,440 --> 00:59:20,440
And we are able to get it to work.

1564
00:59:22,278 --> 00:59:23,278
It's currently very expensive.

1565
00:59:25,558 --> 00:59:26,558
So if you wanna reach a
consumer population, it's-

1566
00:59:34,180 --> 00:59:35,180
- Gotta wait for the
cost curve to come down.

1567
00:59:35,179 --> 00:59:36,179
- Yeah.

1568
00:59:36,196 --> 00:59:37,196
- So that's kind of the
main limiting factor.

1569
00:59:39,634 --> 00:59:40,634
- Well, I think there's that

1570
00:59:40,635 --> 00:59:41,635
and we, yeah, I mean we want
to keep on improving it.

1571
00:59:42,840 --> 00:59:43,840
So I think, but look you learn

1572
00:59:45,979 --> 00:59:46,979
by trying to assemble
and integrate everything.

1573
00:59:49,170 --> 00:59:50,170
You can't just like do a
million R&D efforts in isolation

1574
00:59:54,652 --> 00:59:55,652
and then like hope that
they come together.

1575
00:59:56,875 --> 00:59:57,875
I think part of what
lets you get to building

1576
00:59:59,339 --> 01:00:00,339
the ultimate product is having a few tries

1577
01:00:01,478 --> 01:00:02,478
practicing building the ultimate product.

1578
01:00:03,099 --> 01:00:04,099
And it's like, oh, well
we did that, but I dunno,

1579
01:00:05,490 --> 01:00:06,490
it like wasn't quite as
good on this one dimension

1580
01:00:08,010 --> 01:00:09,010
as we wanted, so let's not ship that one.

1581
01:00:12,177 --> 01:00:13,177
Let's hold that one and
then do the next one.

1582
01:00:14,097 --> 01:00:15,097
So I think that's sort
of some of the process

1583
01:00:16,680 --> 01:00:17,680
that we've had is we have
like multiple generations

1584
01:00:19,860 --> 01:00:20,860
of how we're gonna build this.

1585
01:00:21,900 --> 01:00:22,900
You know, when I look
at the overall budget

1586
01:00:25,057 --> 01:00:26,057
for reality labs I mean it's,

1587
01:00:26,580 --> 01:00:27,580
augmented reality and the
glasses I think is the most

1588
01:00:29,640 --> 01:00:30,640
expensive part of what we're doing.

1589
01:00:31,290 --> 01:00:32,290
- That's why I ask cuz I
think people are wondering

1590
01:00:33,937 --> 01:00:34,937
like where's all this going?

1591
01:00:34,770 --> 01:00:35,770
- No, it's, I mean, but look,

1592
01:00:37,259 --> 01:00:38,259
I think at the end of the day

1593
01:00:38,792 --> 01:00:39,792
I'm quite optimistic about both

1594
01:00:42,150 --> 01:00:43,150
augmented and virtual reality.

1595
01:00:44,838 --> 01:00:45,838
So I think AR glasses are
gonna be the thing that's like

1596
01:00:48,235 --> 01:00:49,235
mobile phones that you walk
around the world wearing.

1597
01:00:51,419 --> 01:00:52,419
VR is gonna be like
your workstation or TV.

1598
01:00:54,000 --> 01:00:55,000
- [Interviewer] Right.

1599
01:00:55,575 --> 01:00:56,575
- Which is when you're like
settling in for a session

1600
01:00:57,960 --> 01:00:58,960
and you want a kind of higher fidelity,

1601
01:01:00,840 --> 01:01:01,840
more compute rich experience,

1602
01:01:04,080 --> 01:01:05,080
then it's gonna be worth putting that on.

1603
01:01:06,097 --> 01:01:07,097
But you're not gonna walk down the street

1604
01:01:07,275 --> 01:01:08,275
wearing VR headset.

1605
01:01:08,598 --> 01:01:09,598
- [Interviewer] Right.

1606
01:01:09,595 --> 01:01:10,595
- I mean, at least I hope not.

1607
01:01:10,619 --> 01:01:11,619
I mean that's not the future
that we're working towards,

1608
01:01:12,337 --> 01:01:13,337
but I do think that
there's somewhat of a bias,

1609
01:01:15,870 --> 01:01:16,870
maybe this is in the tech
industry or maybe overall

1610
01:01:18,450 --> 01:01:19,450
where people think that
the mobile phone one,

1611
01:01:20,130 --> 01:01:21,130
the glasses one is sort
of the only one of the two

1612
01:01:25,470 --> 01:01:26,470
that will end up being valuable,

1613
01:01:27,375 --> 01:01:28,375
but I think like there are a
ton of TVs out there, right?

1614
01:01:32,313 --> 01:01:33,313
And there are a ton of
people who are kind of like,

1615
01:01:34,918 --> 01:01:35,918
spend a lot of time in
front of computers working.

1616
01:01:35,820 --> 01:01:36,820
So I actually think the VR One
will be quite important too.

1617
01:01:40,059 --> 01:01:41,059
But I think that there's no question

1618
01:01:41,970 --> 01:01:42,970
that the larger market over time

1619
01:01:44,400 --> 01:01:45,400
I think should be smart glasses.

1620
01:01:46,977 --> 01:01:47,977
And I mean now I think
you're gonna have both,

1621
01:01:48,690 --> 01:01:49,690
all the immersive quality
of being able to interact

1622
01:01:53,035 --> 01:01:54,035
with people and feel present
no matter where you are

1623
01:01:55,155 --> 01:01:56,155
in sort of a normal form factor

1624
01:01:56,820 --> 01:01:57,820
and you're also gonna have
like the perfect form factor

1625
01:01:59,550 --> 01:02:00,550
to deliver all these AI
experiences over time

1626
01:02:01,830 --> 01:02:02,830
because they'll be able
to see what you see

1627
01:02:03,270 --> 01:02:04,270
and hear what you hear.

1628
01:02:05,031 --> 01:02:06,031
So I don't know.

1629
01:02:05,864 --> 01:02:06,864
I mean it's, yeah, I mean
this stuff is challenging.

1630
01:02:08,817 --> 01:02:09,817
I think making things
small is also very hard.

1631
01:02:13,158 --> 01:02:14,158
- [Interviewer] Yeah.

1632
01:02:14,175 --> 01:02:15,175
- Right, it's like there's this
funny kinda counterintuitive

1633
01:02:16,475 --> 01:02:17,475
thing where I think
humans get super impressed

1634
01:02:18,600 --> 01:02:19,600
by building big things like the pyramids,

1635
01:02:21,090 --> 01:02:22,090
but I think a lot of time
building small things

1636
01:02:23,610 --> 01:02:24,610
like cures for diseases
at a cellular level

1637
01:02:29,280 --> 01:02:30,280
or miniaturizing a super
computer to fit into your glasses

1638
01:02:33,150 --> 01:02:34,150
are like maybe even
bigger feats than building

1639
01:02:36,038 --> 01:02:37,038
some like really physically large things,

1640
01:02:37,380 --> 01:02:38,380
but it like sort of seems a
less impressive for some reason.

1641
01:02:42,385 --> 01:02:43,385
But I don't know, it's
super fascinating stuff.

1642
01:02:44,520 --> 01:02:45,520
- Yeah, I feel like every time we talk

1643
01:02:47,754 --> 01:02:48,754
there's a lot has happened in a year.

1644
01:02:51,218 --> 01:02:52,218
You seem really dialed in
to managing the company.

1645
01:02:53,220 --> 01:02:54,220
And I'm curious kind of what
motivates you these days

1646
01:02:57,254 --> 01:02:58,254
cuz you've got a lot going on

1647
01:02:58,315 --> 01:02:59,315
and you're getting into
fighting, you've got three kids,

1648
01:03:00,235 --> 01:03:01,235
you've got the philanthropy stuff.

1649
01:03:01,595 --> 01:03:02,595
There's a lot going on
and you seem more active

1650
01:03:04,920 --> 01:03:05,920
in kind of day-to-day stuff,
at least externally, than ever.

1651
01:03:09,179 --> 01:03:10,179
You're kind of the last I
think founder of your era

1652
01:03:11,798 --> 01:03:12,798
still leading a company of this large.

1653
01:03:14,715 --> 01:03:15,715
Do you think about that?

1654
01:03:15,718 --> 01:03:16,718
Do you think about kind of
what motivates you still?

1655
01:03:18,737 --> 01:03:19,737
Or is it just kind of still clicking

1656
01:03:19,570 --> 01:03:20,570
and it's kind of more subconscious?

1657
01:03:23,275 --> 01:03:24,275
- Yeah, I don't know.

1658
01:03:24,278 --> 01:03:25,278
I mean I'm not sure that
that much of the stuff

1659
01:03:25,111 --> 01:03:26,111
that you said is that new?

1660
01:03:27,120 --> 01:03:28,120
I mean kids are, seven years
old, almost eight now, right?

1661
01:03:32,817 --> 01:03:33,817
So that's been for a while.

1662
01:03:35,796 --> 01:03:36,796
Yeah, the fighting thing is relatively

1663
01:03:36,629 --> 01:03:37,629
new over the last few years,

1664
01:03:38,216 --> 01:03:39,216
but I've always been very physical.

1665
01:03:39,355 --> 01:03:40,355
But I dunno, we go through different waves

1666
01:03:43,935 --> 01:03:44,935
in terms of like what the
company needs to be doing

1667
01:03:51,259 --> 01:03:52,259
and I think that that calls for

1668
01:03:52,838 --> 01:03:53,838
somewhat different styles of leadership.

1669
01:03:55,056 --> 01:03:56,056
And I think we went through
a period where a lot of what

1670
01:03:59,550 --> 01:04:00,550
we needed to do was tackle and navigate

1671
01:04:03,720 --> 01:04:04,720
some important social
issues and I think that that

1672
01:04:06,657 --> 01:04:07,657
required a somewhat different style.

1673
01:04:09,537 --> 01:04:10,537
And then we went through
a period where we had

1674
01:04:10,500 --> 01:04:11,500
some quite big business
challenges handling in a recession

1675
01:04:14,310 --> 01:04:15,310
and revenue not coming in
the way that we thought

1676
01:04:17,040 --> 01:04:18,040
and needing to do layoffs

1677
01:04:20,177 --> 01:04:21,177
and that required a
somewhat different style.

1678
01:04:22,710 --> 01:04:23,710
But now I think we're
squarely back in developing

1679
01:04:28,379 --> 01:04:29,379
really innovative products,

1680
01:04:29,899 --> 01:04:30,899
especially because of some
of the innovations in AI.

1681
01:04:33,398 --> 01:04:34,398
I think that in some ways that
just like plays exactly to,

1682
01:04:34,916 --> 01:04:35,916
I think my favorite style
of running a company.

1683
01:04:40,198 --> 01:04:41,198
But I don't know, I think
these things evolve over time.

1684
01:04:42,420 --> 01:04:43,420
- It seems like you're having more fun.

1685
01:04:45,132 --> 01:04:46,132
- Well, how can you not?

1686
01:04:46,137 --> 01:04:47,137
I mean this is like, I
mean I think what's great

1687
01:04:48,379 --> 01:04:49,379
about the tech industry is
like every once in a while

1688
01:04:50,779 --> 01:04:51,779
you get something like
these AI breakthroughs

1689
01:04:52,440 --> 01:04:53,440
and it just changes
everything and like yeah,

1690
01:04:55,127 --> 01:04:56,127
I mean that can be threatening
if you're behind it,

1691
01:04:58,470 --> 01:04:59,470
but I just think that that's
like when stuff changes

1692
01:05:00,660 --> 01:05:01,660
and when awesome stuff gets built.

1693
01:05:02,894 --> 01:05:03,894
So maybe that's exciting.

1694
01:05:04,134 --> 01:05:05,134
I don't know.

1695
01:05:05,100 --> 01:05:06,100
I mean I guess personally,

1696
01:05:07,099 --> 01:05:08,099
I think a lot of people,

1697
01:05:11,995 --> 01:05:12,995
I mean the world has been so weird

1698
01:05:13,238 --> 01:05:14,238
over the last few years, right?

1699
01:05:14,518 --> 01:05:15,518
Especially going back
to the COVID pandemic

1700
01:05:17,220 --> 01:05:18,220
and all that stuff.

1701
01:05:18,974 --> 01:05:19,974
And I think that that was an
opportunity for a lot of people

1702
01:05:21,270 --> 01:05:22,270
to just sort of like reassess
what they found meaningful

1703
01:05:24,497 --> 01:05:25,497
in their lives.

1704
01:05:25,718 --> 01:05:26,718
And there's obviously a lot of stuff

1705
01:05:27,217 --> 01:05:28,217
that was tough about it,

1706
01:05:28,115 --> 01:05:29,115
but the silver lining is I
got to spend a lot more time

1707
01:05:29,760 --> 01:05:30,760
with my family and we got
to just like I could spend

1708
01:05:34,875 --> 01:05:35,875
more time out in nature cuz
I like wasn't coming into

1709
01:05:36,854 --> 01:05:37,854
the office quite as much
and like it was definitely

1710
01:05:40,838 --> 01:05:41,838
a period of reflection where I sort of,

1711
01:05:43,410 --> 01:05:44,410
I felt like since the
time I was basically,

1712
01:05:47,070 --> 01:05:48,070
I was like 19 when I started the company.

1713
01:05:49,574 --> 01:05:50,574
And like every year it's just, okay,

1714
01:05:50,407 --> 01:05:51,407
we want to connect more people, right?

1715
01:05:52,577 --> 01:05:53,577
It's like connecting people is good.

1716
01:05:54,450 --> 01:05:55,450
That's sort of what we're here to do.

1717
01:05:55,653 --> 01:05:56,653
Let's like make this bigger and bigger

1718
01:05:57,030 --> 01:05:58,030
and kind of connect more people

1719
01:06:00,570 --> 01:06:01,570
and build more products that
allow people to do that.

1720
01:06:06,449 --> 01:06:07,449
And I guess we just sort of hit the scale

1721
01:06:08,134 --> 01:06:09,134
where I think to me what I
found sort of satisfaction

1722
01:06:15,718 --> 01:06:16,718
in life from and what I think
is like the right strategy,

1723
01:06:20,492 --> 01:06:21,492
I think both for like me
personally and for the company

1724
01:06:22,380 --> 01:06:23,380
is less to just focus on
like, okay we're gonna just

1725
01:06:25,650 --> 01:06:26,650
like connect more people and more like,

1726
01:06:27,960 --> 01:06:28,960
let's do some awesome things and-

1727
01:06:32,534 --> 01:06:33,534
- Sounds very technical.

1728
01:06:34,257 --> 01:06:35,257
- I mean there are a lot of
different analogies on this,

1729
01:06:36,854 --> 01:06:37,854
but I mean someone made this
point to me that doing good

1730
01:06:41,798 --> 01:06:42,798
things is different from
doing awesome things.

1731
01:06:44,657 --> 01:06:45,657
And like in social media
in a lot of ways it's good,

1732
01:06:47,899 --> 01:06:48,899
right, like gives a lot of people a voice

1733
01:06:49,500 --> 01:06:50,500
and it lets them connect
and it's like sort of warm

1734
01:06:53,958 --> 01:06:54,958
and it's taking like a basic technology

1735
01:06:55,110 --> 01:06:56,110
and bringing it to billions of people,

1736
01:06:57,638 --> 01:06:58,638
but and that there's
an inherent awesomeness

1737
01:07:00,795 --> 01:07:01,795
of like doing some technical
feat for the first time.

1738
01:07:05,915 --> 01:07:06,915
And I guess I'm for the
next phase of what we do,

1739
01:07:10,417 --> 01:07:11,417
just a little more focused on that.

1740
01:07:12,594 --> 01:07:13,594
I think we've done a lot of good things.

1741
01:07:15,752 --> 01:07:16,752
I think we need to make sure
that they stay good, right?

1742
01:07:18,795 --> 01:07:19,795
I think that there's like a lot
of work that needs to happen

1743
01:07:20,190 --> 01:07:21,190
on making sure the balance
of all that is right.

1744
01:07:23,457 --> 01:07:24,457
But for the next wave of,

1745
01:07:25,915 --> 01:07:26,915
I guess my life and for the company,

1746
01:07:27,600 --> 01:07:28,600
but also outside of the company,

1747
01:07:30,090 --> 01:07:31,090
what I'm doing at CZI and it
was just some of my personal

1748
01:07:33,274 --> 01:07:34,274
projects It's like I sort of
define my life at this point

1749
01:07:36,270 --> 01:07:37,270
more in terms of getting
to work on awesome things

1750
01:07:39,660 --> 01:07:40,660
with great people who I like working with.

1751
01:07:42,737 --> 01:07:43,737
So it's like I work on
all this reality lab stuff

1752
01:07:49,137 --> 01:07:50,137
with Baz and a team over there

1753
01:07:49,970 --> 01:07:50,970
and it's just super exciting

1754
01:07:51,330 --> 01:07:52,330
and I get to work on all this AI stuff

1755
01:07:53,310 --> 01:07:54,310
and it with Chris and
Ahmed and like the folks

1756
01:07:56,580 --> 01:07:57,580
who are working on that
and it's really exciting

1757
01:07:59,676 --> 01:08:00,676
and like we get to work on
some of the philanthropy work

1758
01:08:02,160 --> 01:08:03,160
and helping to cure
diseases with Priscilla

1759
01:08:04,470 --> 01:08:05,470
and a lot of the best
scientists in the world

1760
01:08:07,201 --> 01:08:08,201
and that's really cool.

1761
01:08:08,198 --> 01:08:09,198
And it's like, okay, and it's like,

1762
01:08:09,739 --> 01:08:10,739
so just then there's like personal stuff.

1763
01:08:10,572 --> 01:08:11,572
It's like we get to raise a family.

1764
01:08:12,518 --> 01:08:13,518
It's like that's really neat

1765
01:08:13,878 --> 01:08:14,878
and like there's no other
person I'd rather do that with.

1766
01:08:16,177 --> 01:08:17,177
And I don't know, to me that's just sort

1767
01:08:18,450 --> 01:08:19,450
of where I am in life now.

1768
01:08:21,597 --> 01:08:22,597
- Sounds like a nice place to be.

1769
01:08:23,217 --> 01:08:24,217
- Ah, I mean I'm enjoying it.

1770
01:08:24,737 --> 01:08:25,737
- Mark Zuckerberg, the optimist.

1771
01:08:26,775 --> 01:08:27,775
- I mean, always somewhat optimistic.

1772
01:08:27,608 --> 01:08:28,608
- Yeah, thanks for the time, Mark.

1773
01:08:29,558 --> 01:08:30,558
- Yeah, appreciate it.

1774
01:08:30,577 --> 01:08:31,577
- Thank you.